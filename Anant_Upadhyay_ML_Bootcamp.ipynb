{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GY8xTX7ieW9Y"
      },
      "source": [
        "#Prerequisites"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0lxTsnQVAnmS"
      },
      "outputs": [],
      "source": [
        "#First we import all the required libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UShqGtVBYX6s"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bmFEJWR6_yB6"
      },
      "source": [
        "# **Linear Regression**\n",
        "In this part we will train a model linear regression model with multiple features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HeJSk8MHBAQW"
      },
      "outputs": [],
      "source": [
        "#Now we load and print our training data\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/linear_train.csv\") #Loading data from google drive\n",
        "linear_train_data = np.array(df) #Converting loaded data in a matrix\n",
        "X_train = linear_train_data[:,1:21] #Extracting X_train(Input data)\n",
        "Y_train = linear_train_data[:,21] ##Extracting Y_train(Labels)\n",
        "Y_train = Y_train.reshape(Y_train.size,1)\n",
        "# Printing the first 5 rows of our loaded data\n",
        "print(\"Input\\n\",X_train[:5])\n",
        "print(\"labels\\n\",Y_train[:5])\n",
        "print(X_train.shape)\n",
        "print(Y_train.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r01rXEiMNKD3"
      },
      "source": [
        "## Creating the model\n",
        "As we are making a linear model, we will first initialize a random array **w** having n = 20 features and a random scalar value **b**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DGjlrC0kOLHo"
      },
      "outputs": [],
      "source": [
        "# Initialize initial w and b (having small values)\n",
        "np.random.seed(1) #Used so that we get the same random state on multiple executions of code\n",
        "w_initial = 0.01*(np.random.randn(20,1)-0.5)\n",
        "b_initial = np.random.randn()\n",
        "print(w_initial)\n",
        "print(b_initial)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ZtX4KSeRngp"
      },
      "source": [
        "##Predict function\n",
        "We know f_wb is given as f_wb = **w** * **x** + b\n",
        "Here we create a predict function to make a prediction on the given data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JIP3b-vCT6tH"
      },
      "outputs": [],
      "source": [
        "def predict(X,w,b):\n",
        "  \"\"\"\n",
        "  Arguments\n",
        "  X(2D numpy array of training examples) Shape(m,n)\n",
        "  w(numpy array of model parameters) Shape(n,1)\n",
        "  b(model parameter) scalar \n",
        "  \"\"\"\n",
        "  m,n = X.shape\n",
        "  a = np.matmul(X,w)\n",
        "  a = a.reshape(m,1)\n",
        "  f_wb = a + b\n",
        "  return f_wb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gu2aqyJoWCdc"
      },
      "source": [
        "Now lets test our model on the first 5 training examples of our data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MNRvONdNVl1N"
      },
      "outputs": [],
      "source": [
        "  f_wb = predict(X_train[:5],w_initial,b_initial)\n",
        "  print(\"Prediction:-\\n\",f_wb)\n",
        "  print(\"Expected Output:-\\n\",Y_train[:5])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EiKWDPcqb_A-"
      },
      "source": [
        "##Computing Cost\n",
        "As you can see the prediction is far off from the expected output so now we compute the cost function , as a metric to measure how far off is our model from the training examples "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CkhSJzJYYeIx"
      },
      "outputs": [],
      "source": [
        "def compute_cost(X,Y,w,b):\n",
        "  \"\"\"\n",
        "   Arguments\n",
        "   X(2-D numpy array containing the training examples) Shape(m,n)\n",
        "   Y(1-D numpy array containing the output label of all the training examples) Shape(m,1)\n",
        "   w(numpy array of model parameters) Shape(n,1)\n",
        "   b(model parameter) scalar \n",
        "  \"\"\"\n",
        "  m = len(Y) #Getting the number of training examples\n",
        "  f_wb = predict(X,w,b)\n",
        "  err = (f_wb - Y)**2\n",
        "  err = err.reshape(m,1)\n",
        "  cost = np.sum(err) # specific cost for each training example is added and divided by 2m to give final cost\n",
        "  cost = cost/(2*m)\n",
        "  return cost"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9cX5VHkIbrp5"
      },
      "source": [
        "Computing Cost of our initial prediction:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iSUaWtZ9bxMe"
      },
      "outputs": [],
      "source": [
        "J = compute_cost(X_train,Y_train,w_initial,b_initial)\n",
        "print(\"Cost:\",J)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2imiIuq9cPl9"
      },
      "source": [
        "##Gradient descent\n",
        "After knowing the cost of the model we know define a function to reduce the cost using gradient descent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tS7e_yJtcxON"
      },
      "outputs": [],
      "source": [
        "def gradient_descent(X,Y,w_in,b_in,alpha,iters,predict,compute_cost):\n",
        "  \n",
        "  J_history = [] #We create a list containg cost after every iteration(For later plotting and analysis)\n",
        "  w = w_in.copy() \n",
        "  b = b_in\n",
        "  m,n = X.shape #Getting the number of training examples\n",
        "  for i in range(iters):\n",
        "    #Compute gradient dj_dw and dj_db\n",
        "    f_wb = predict(X,w,b)\n",
        "    err = f_wb - Y\n",
        "    dj_dw = (np.matmul(err.T,X).T)/m\n",
        "    dj_db = np.sum(err)/m \n",
        "    #Update parameters w,b (Simultaneously)\n",
        "    w = w-alpha*dj_dw\n",
        "    b = b-alpha*dj_db\n",
        "    #Record Cost J after every iteration\n",
        "    J_history.append(compute_cost(X, Y, w, b)) #Add the current cost to J_history\n",
        "    # Print cost after interval of 10 times,[-1] refers to the last element(reverse indexing)\n",
        "    if i%(np.math.ceil(iters / 10)) == 0:\n",
        "      print(\"Iteration :\",i,\" Cost :\",J_history[-1]) #F-strings used to encode variables i and J_history  \n",
        "  return w, b, J_history #return final w,b and J_history(for graphing)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hloEumdkU76F"
      },
      "source": [
        "##Tuning Hyperparameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MnT2O6L7dG47"
      },
      "source": [
        "###Choosing the learning rate(alpha)\n",
        "Let's start by choosing the learning rate alpha by trying 3 examples 1e-7 , 1e-8 and 1e-6 and see which fits best"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pex-H4QUX0Ng"
      },
      "outputs": [],
      "source": [
        "##For alpha = 1e-7\n",
        "w_final,b_final,J_history = gradient_descent(X_train,Y_train,w_initial,b_initial,0.0000001,1000,predict,compute_cost)\n",
        "plt.figure(figsize=(12,4))\n",
        "plt.plot(J_history)\n",
        "plt.title(\"Cost v.s iteration\")\n",
        "plt.ylabel(\"Cost\")\n",
        "plt.xlabel(\"Number of iterations\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gMgpMCcIY8nx"
      },
      "outputs": [],
      "source": [
        "# Now trying for alpha = 1e-6 and reduce the number of iterations to 100 (will be justified)\n",
        "w_final,b_final,J_history = gradient_descent(X_train,Y_train,w_initial,b_initial,0.000001,100,predict,compute_cost)\n",
        "plt.figure(figsize=(12,4))\n",
        "plt.plot(J_history)\n",
        "plt.title(\"Cost v.s iteration\")\n",
        "plt.ylabel(\"Cost\")\n",
        "plt.xlabel(\"Number of iterations\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hc2lWa_Vb1_3"
      },
      "source": [
        "As you can see the model is clearly overshooting,the cost keeps on increasing , so we can conclude the learning rate is too high and we don't need to go above 100 iterations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zEGJVfE8cf2Z"
      },
      "outputs": [],
      "source": [
        "# Now trying for alpha = 1e-8 \n",
        "w_final,b_final,J_history = gradient_descent(X_train,Y_train,w_initial,b_initial,0.00000001,1000,predict,compute_cost)\n",
        "plt.figure(figsize=(12,4))\n",
        "plt.plot(J_history)\n",
        "plt.title(\"Cost v.s iteration\")\n",
        "plt.ylabel(\"Cost\")\n",
        "plt.xlabel(\"Number of iterations\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q30crM71en-t"
      },
      "source": [
        "As you can see after some iterations the the decrease in the cost function decreases very slowly,although higher number of iterations might further decrease the cost function it would be computationaly intesive and higher learning rate might lead to faster convergence."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0PclD1Lcf2XA"
      },
      "source": [
        "From the above results we can conclude the learning rate 1e-7 is ideal for our model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GfeHHR7zgJpF"
      },
      "source": [
        "###Deciding the number of iterations\n",
        "Let's try 3 different combinations, no.of iteration = 100 ,1000 and 10000,100000 and measure performance in each of the 4 test cases"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5KsN3QYcgVDf"
      },
      "outputs": [],
      "source": [
        "##For 100 iterations\n",
        "w_final,b_final,J_history = gradient_descent(X_train,Y_train,w_initial,b_initial,0.0000001,100,predict,compute_cost)\n",
        "plt.figure(figsize=(12,4))\n",
        "plt.plot(J_history)\n",
        "plt.title(\"Cost v.s iteration\")\n",
        "plt.ylabel(\"Cost\")\n",
        "plt.xlabel(\"Number of iterations\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DxhszWHfli6y"
      },
      "source": [
        "Here it seems the function could still be decreasing further and might yield better results at higher number of iterations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vO0kNaDel4eF"
      },
      "outputs": [],
      "source": [
        "##For 1000 iterations\n",
        "w_final,b_final,J_history = gradient_descent(X_train,Y_train,w_initial,b_initial,0.0000001,1000,predict,compute_cost)\n",
        "plt.figure(figsize=(12,4))\n",
        "plt.plot(J_history)\n",
        "plt.title(\"Cost v.s iteration\")\n",
        "plt.ylabel(\"Cost\")\n",
        "plt.xlabel(\"Number of iterations\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pTnJfFK9maZy"
      },
      "source": [
        "Here it seems that the cost function has pretty much converged, however let's zoom in to the tail end of the plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s2ky9mUIqJEW"
      },
      "outputs": [],
      "source": [
        "fig, (ax1, ax2) = plt.subplots(1, 2, constrained_layout=True, figsize=(12,4)) #We create 2 subplots ax1 and ax2 to see the cost at start and end\n",
        "ax1.plot(J_history[:200]) #1st subplot plots till the first 100 examples\n",
        "ax2.plot(J_history[200:]) #2nd subplot plots on the examples after 200 iterations\n",
        "ax1.set_title(\"Cost vs. iteration(start)\");  ax2.set_title(\"Cost vs. iteration (end)\")\n",
        "ax1.set_ylabel('Cost')            ;  ax2.set_ylabel('Cost') \n",
        "ax1.set_xlabel('Number of iterations')  ;  ax2.set_xlabel('Number of iterations') \n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KoGRJ4r2szin"
      },
      "source": [
        "It seems the cost function could further decrease if we increased the number of iterations. Let's try 10000 iterations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hQFVfunntRnc"
      },
      "outputs": [],
      "source": [
        "##For 10000 iterations\n",
        "w_final,b_final,J_history = gradient_descent(X_train,Y_train,w_initial,b_initial,0.0000001,10000,predict,compute_cost)\n",
        "plt.figure(figsize=(12,4))\n",
        "plt.plot(J_history)\n",
        "plt.title(\"Cost v.s iteration\")\n",
        "plt.ylabel(\"Cost\")\n",
        "plt.xlabel(\"Number of iterations\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fztbsIntt5-D"
      },
      "source": [
        "As we can see that our cost function has come down significantly lets try to go even higher and run our model for 100,000 iterations and see the improvements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rIqmidzJD-sL"
      },
      "outputs": [],
      "source": [
        "##For 100000 iterations\n",
        "w_final,b_final,J_history = gradient_descent(X_train,Y_train,w_initial,b_initial,0.0000001,100000,predict,compute_cost)\n",
        "plt.figure(figsize=(12,4))\n",
        "plt.plot(J_history)\n",
        "plt.title(\"Cost v.s iteration\")\n",
        "plt.ylabel(\"Cost\")\n",
        "plt.xlabel(\"Number of iterations\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AoedU5tiFxHR"
      },
      "source": [
        "Now we will take the pretrained parameters w and b and again run gradient descent for 100,000 iterations to see if the cost further decreases significantly"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JAmVw1dcGN3_"
      },
      "outputs": [],
      "source": [
        "#Using pretrained parameters we run the gradient descent for another 100000 iterations\n",
        "w_final,b_final,J_history = gradient_descent(X_train,Y_train,w_final,b_final,0.0000001,100000,predict,compute_cost)\n",
        "plt.figure(figsize=(12,4))\n",
        "plt.plot(J_history)\n",
        "plt.title(\"Cost v.s iteration\")\n",
        "plt.ylabel(\"Cost\")\n",
        "plt.xlabel(\"Number of iterations\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hJRusWUpVgJy"
      },
      "source": [
        "We can see the cost function saturates and after a point,and there is no point in having higher number of iterations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tj6qvSYii_-N"
      },
      "source": [
        "###Visualization of parameters\n",
        "Here we will plot 20 different plots depicting the ith feature of X against the output label y(Predicted by our model) and we will also show the actual training data on the plot. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "orjx1r8elXar"
      },
      "outputs": [],
      "source": [
        "for i in range(20):\n",
        "  plt.scatter(X_train[:,i],Y_train, marker='.',c='r',label=\"Training data\")\n",
        "  plt.xlabel(f\"Feature no:-{i+1}\")\n",
        "  plt.ylabel(\"Output\")\n",
        "  a = np.zeros(20)\n",
        "  a[i] = 1\n",
        "  a = a.reshape(20,1)\n",
        "  f_wb = predict(X_train,a*w_final,b_final)\n",
        "  plt.plot(X_train[:,i],f_wb,label=\"Prediction\")\n",
        "  plt.legend()\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TofdYrJtuv4M"
      },
      "source": [
        "##Feature scaling\n",
        "Taking a closer look at the data we find out that the data in X_train is spread across a huge range (from near 0 to upto 1000).It seems that applying z-score normalization to all the features of x will allow it to come under similar ranges and hence give us a better chance at making more accurate predictions.(**Note:- Even the test data must be z-score normalized before being used as input for the model**)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f32m1cpNwQvF"
      },
      "outputs": [],
      "source": [
        "def z_score_normalization(X):\n",
        "  \"\"\"\n",
        "  Arguments\n",
        "  X(the input data which is to be normalized): Shape (m,n) \n",
        "  \"\"\"\n",
        "  # First get no.of training examples and features\n",
        "  m,n = (X).shape\n",
        "  mean = np.sum(X,axis=0)/m #Has the mean of each of the n features stored in a 1-D array\n",
        "  mean = mean.reshape(1,-1)\n",
        "  sq_X = X**2 #Squares the matrix(element wise)\n",
        "  sq_mean = np.sum(sq_X,axis=0)/m #Mean of each of the n features in the squared matrix\n",
        "  sq_mean = sq_mean.reshape(1,-1)\n",
        "  std_dev = np.sqrt(sq_mean - (mean)**2) #Formula for standard deviation\n",
        "  std_dev = std_dev.reshape(1,-1)\n",
        "  ##Finally the normalized data can be written as\n",
        "  X_normalized = (X - mean)/std_dev #here broadcasting is used to make these operations possible \n",
        "  return X_normalized, mean, std_dev"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BjiYQVdF2Du4"
      },
      "source": [
        "Let's print the first 5 rows of our new normalized data!!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bxxsuzbN2Mzt"
      },
      "outputs": [],
      "source": [
        "X_train_normalized,u,s = z_score_normalization(X_train)\n",
        "print(\"Data:-\",X_train_normalized[:5])\n",
        "print(\"Mean:-\",u)\n",
        "print(\"std_dev:-\",s)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NNldR1nD2s55"
      },
      "source": [
        "As you can see after normalization most of our training data belongs to the range -3 to 3 which makes gradient descent much easier.Let's train our model on the new normalized data!!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8xnV190-293U"
      },
      "outputs": [],
      "source": [
        "#Training the model on 10000 iterations\n",
        "w_final,b_final,J_history = gradient_descent(X_train_normalized,Y_train,w_initial,b_initial,0.001,10000,predict,compute_cost)\n",
        "plt.figure(figsize=(12,4))\n",
        "plt.plot(J_history)\n",
        "plt.title(\"Cost v.s iteration\")\n",
        "plt.ylabel(\"Cost\")\n",
        "plt.xlabel(\"Number of iterations\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3j7JfjzH4gO5"
      },
      "source": [
        "The cost decreases very slowly in the end so we will try a higher learning rate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3jQgFe_s4y1F"
      },
      "outputs": [],
      "source": [
        "w_final,b_final,J_history = gradient_descent(X_train_normalized,Y_train,w_initial,b_initial,0.1,10000,predict,compute_cost)\n",
        "plt.figure(figsize=(12,4))\n",
        "plt.plot(J_history)\n",
        "plt.title(\"Cost v.s iteration\")\n",
        "plt.ylabel(\"Cost\")\n",
        "plt.xlabel(\"Number of iterations\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IzXQpXUl0fSk"
      },
      "outputs": [],
      "source": [
        "print(w_final)\n",
        "print(b_final)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cCZsYI-DmaC7"
      },
      "source": [
        "###Visualizing model after feature scaling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C-XDI-mXLd4S"
      },
      "outputs": [],
      "source": [
        "for i in range(20):\n",
        "  plt.scatter(X_train_normalized[:,i],Y_train, marker='.',c='r',label = \"Training data\")\n",
        "  plt.xlabel(f\"Feature no:-{i+1}\")\n",
        "  plt.ylabel(\"Output\")\n",
        "  a = np.zeros(20)\n",
        "  a[i] = 1\n",
        "  a = a.reshape(20,1)\n",
        "  f_wb = predict(X_train_normalized,a*w_final,b_final)\n",
        "  plt.plot(X_train_normalized[:,i],f_wb, label=\"Prediction\")\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HJkQ3dB0wi7m"
      },
      "source": [
        "##Creating a cross validation set\n",
        "No, matter how good our model does on the training set,there is always chance of overfitting,so we create a cross validation test,ratio of test to train is 80% train,20% test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VR30kirl5xN0"
      },
      "outputs": [],
      "source": [
        "def train_test_split(X,Y,ratio):\n",
        "    #ratio is the ratio of the train set to that of complete set\n",
        "    m,n = X.shape\n",
        "    a = m*ratio\n",
        "    a = round(a)\n",
        "    Z = np.append(X,Y,axis=1)\n",
        "    np.random.seed(261)\n",
        "    np.random.shuffle(Z)\n",
        "    X = Z[:,:-1]\n",
        "    Y = Z[:,-1]\n",
        "    Y = Y.reshape(Y.size,1)\n",
        "    X_train_r = X[:a]\n",
        "    X_cv_r = X[a:]\n",
        "    Y_train_r = Y[:a]\n",
        "    Y_cv_r = Y[a:]\n",
        "    return X_train_r,X_cv_r,Y_train_r,Y_cv_r"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5K-EwRM77aK-"
      },
      "outputs": [],
      "source": [
        "X_train_r,X_cv_r,Y_train_r,Y_cv_r = train_test_split(X_train_normalized,Y_train,0.8)\n",
        "print(X_train_r.shape)\n",
        "print(Y_train_r.shape)\n",
        "print(X_cv_r.shape)\n",
        "print(Y_cv_r.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xhwFT2NB7_YJ"
      },
      "source": [
        "Now we define another gradient descent function to account for J_cv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "50P0O2ni9Cl7"
      },
      "outputs": [],
      "source": [
        "def gradient_descent_cv(X,Y,w_in,b_in,alpha,iters,ratio,predict,compute_cost):\n",
        "  \n",
        "  J_train_history = [] #We create a list containg cost after every iteration(For later plotting and analysis)\n",
        "  J_cv_history = []\n",
        "  w = w_in.copy() \n",
        "  b = b_in\n",
        "  X_train,X_cv,Y_train,Y_cv = train_test_split(X,Y,ratio)\n",
        "  m,n = X_train.shape #Getting the number of training examples\n",
        "  for i in range(iters):\n",
        "    #Compute gradient dj_dw and dj_db\n",
        "    f_wb = predict(X_train,w,b)\n",
        "    err = f_wb - Y_train\n",
        "    dj_dw = (np.matmul(err.T,X_train).T)/m\n",
        "    dj_db = np.sum(err)/m \n",
        "    #Update parameters w,b (Simultaneously)\n",
        "    w = w-alpha*dj_dw\n",
        "    b = b-alpha*dj_db\n",
        "    #Record Cost J after every iteration\n",
        "    J_train_history.append(compute_cost(X_train, Y_train, w, b)) #Add the current cost to J_history\n",
        "    J_cv_history.append(compute_cost(X_cv, Y_cv, w, b))\n",
        "    # Print cost after interval of 10 times,[-1] refers to the last element(reverse indexing)\n",
        "    if i%(np.math.ceil(iters / 10)) == 0:\n",
        "      print(\"Iteration :\",i,\" Cost :\",J_train_history[-1]) #F-strings used to encode variables i and J_history  \n",
        "  return w, b, J_train_history,J_cv_history #return final w,b and J_history(for graphing)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lyeZLtj4_d9L"
      },
      "outputs": [],
      "source": [
        "w_final,b_final,J_train_history,J_cv_history = gradient_descent_cv(X_train_normalized,Y_train,w_initial,b_initial,0.01,1000,0.8,predict,compute_cost)\n",
        "plt.figure(figsize=(12,4))\n",
        "plt.plot(J_train_history , label = \"J train\")\n",
        "plt.plot(J_cv_history , label = \"J cv\")\n",
        "plt.title(\"Cost v.s iteration\")\n",
        "plt.ylabel(\"Cost\")\n",
        "plt.xlabel(\"Number of iterations\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s2cunh-QHuRf"
      },
      "source": [
        "J train and J cv are almost overlapping"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mWIuIFARH4sX"
      },
      "source": [
        "##Regularization\n",
        "Now we finally introduce a regularized model(to ensure the values of weights are small enough to not overfit on the data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U3m_YIYNJtZ-"
      },
      "outputs": [],
      "source": [
        "def compute_cost_l(X,Y,w,b,l):\n",
        "  \"\"\"\n",
        "   Arguments\n",
        "   X(2-D numpy array containing the training examples) Shape(m,n)\n",
        "   Y(1-D numpy array containing the output label of all the training examples) Shape(m,1)\n",
        "   w(numpy array of model parameters) Shape(n,1)\n",
        "   b(model parameter) scalar \n",
        "  \"\"\"\n",
        "  m = len(Y) #Getting the number of training examples\n",
        "  f_wb = predict(X,w,b)\n",
        "  err = (f_wb - Y)**2\n",
        "  err = err.reshape(m,1)\n",
        "  cost = np.sum(err) # specific cost for each training example is added and divided by 2m to give final cost\n",
        "  cost = cost/(2*m)\n",
        "  cost += (l/2*m)*np.sum(w**2)\n",
        "  #print(cost,\"Cost of model\")\n",
        "  return cost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6b1Iq-t7KY50"
      },
      "outputs": [],
      "source": [
        "def gradient_descent_cv_l(X,Y,w_in,b_in,alpha,iters,ratio,l,predict,compute_costs):\n",
        "  J_train_history = [] #We create a list containg cost after every iteration(For later plotting and analysis)\n",
        "  J_cv_history = []\n",
        "  w = w_in.copy() \n",
        "  b = b_in\n",
        "  X_train,X_cv,Y_train,Y_cv = train_test_split(X,Y,ratio)\n",
        "  m,n = X_train.shape #Getting the number of training examples\n",
        "  for i in range(iters):\n",
        "    #Compute gradient dj_dw and dj_db\n",
        "    f_wb = predict(X_train,w,b)\n",
        "    err = f_wb - Y_train\n",
        "    dj_dw = (np.matmul(err.T,X_train).T)/m\n",
        "    dj_dw += (l/m)*w\n",
        "    dj_db = np.sum(err)/m \n",
        "    #Update parameters w,b (Simultaneously)\n",
        "    w = w-alpha*dj_dw\n",
        "    b = b-alpha*dj_db\n",
        "    #Record Cost J after every iteration\n",
        "    #Note:- Even though we have applied regularization the method for computing training and testing error is still same\n",
        "    J_train_history.append(compute_costs(X_train, Y_train, w, b)) #Add the current cost to J_history\n",
        "    J_cv_history.append(compute_costs(X_cv, Y_cv, w, b))\n",
        "    # Print cost after interval of 10 times,[-1] refers to the last element(reverse indexing)\n",
        "    if i%(np.math.ceil(iters / 10)) == 0:\n",
        "      print(\"Iteration :\",i,\" Cost :\",J_train_history[-1]) #F-strings used to encode variables i and J_history  \n",
        "  return w, b, J_train_history,J_cv_history #return final w,b and J_history(for graphing)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E9U6-tVALHzz"
      },
      "outputs": [],
      "source": [
        "#Final answer\n",
        "w_ans,b_ans,J_train_history,J_cv_history = gradient_descent_cv_l(X_train_normalized,Y_train,w_initial,b_initial,0.01,10000,0.8,0.000001,predict,compute_cost)\n",
        "plt.figure(figsize=(12,4))\n",
        "plt.plot(J_train_history , label = \"J train\")\n",
        "plt.plot(J_cv_history , label = \"J cv\")\n",
        "plt.title(\"Cost v.s iteration\")\n",
        "plt.ylabel(\"Cost\")\n",
        "plt.xlabel(\"Number of iterations\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oBUPAFVULLG_"
      },
      "source": [
        "J-train and J-cv are almost overlapping"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9uqKg3PdrYcU"
      },
      "source": [
        "##Calculating accuracy of model (Using R_2 square method)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "02W8FSLNrthK"
      },
      "outputs": [],
      "source": [
        "def compute_R_2(w,b,X,Y,compute_cost):\n",
        "  m,n = Y.shape\n",
        "  mean = np.sum(Y)/m\n",
        "  sq_Y = Y**2 \n",
        "  sq_mean = np.sum(sq_Y)/m \n",
        "  var_y = sq_mean - (mean)**2 \n",
        "  r_2 = 1 -2*(compute_cost(X,Y,w,b))/(var_y)\n",
        "  return r_2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L-b1lShDtH8Y"
      },
      "outputs": [],
      "source": [
        "#Computing the accuracy of our trained model\n",
        "r_2 = compute_R_2(w_ans,b_ans,X_train_normalized,Y_train,compute_cost)\n",
        "print(\"Accuracy:\",r_2*100,\"%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VSKp6oxYMuc9"
      },
      "source": [
        "##Generalizing Linear regression for n features (USING CLASSES)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DLHFFsNxFj0X"
      },
      "source": [
        "The code below will be later used to make my own python library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SpPo3XxnOF74"
      },
      "outputs": [],
      "source": [
        "class LinearRegression:\n",
        "  def __init__(self,X,Y,alpha,l,iters):\n",
        "    self.X = X\n",
        "    self.Y = Y\n",
        "    m,n = X.shape\n",
        "    self.m = m\n",
        "    self.n = n\n",
        "    self.alpha = alpha\n",
        "    self.l = l\n",
        "    self.iters = iters\n",
        "    np.random.seed(2)\n",
        "    self.w = 0.01*(np.random.randn(self.n,1)-0.5)\n",
        "    self.b = np.random.randn()\n",
        "    self.J_history = []\n",
        "  \n",
        "  def predict(self):\n",
        "    a = np.matmul(self.X,self.w)\n",
        "    a = a.reshape(self.m,1)\n",
        "    f_wb = a + self.b\n",
        "    return f_wb\n",
        "  \n",
        "  def compute_cost(self):\n",
        "    f_wb = self.predict()\n",
        "    err = (f_wb - self.Y)**2\n",
        "    err = err.reshape(self.m,1)\n",
        "    cost = np.sum(err) \n",
        "    cost = cost/(2*(self.m))\n",
        "    return cost\n",
        "  \n",
        "  def gradient_descent(self):\n",
        "    J_history = [] \n",
        "    for i in range(self.iters):\n",
        "      f_wb = self.predict()\n",
        "      err = f_wb - self.Y\n",
        "      dj_dw = (np.matmul(err.T,self.X).T)/(self.m)\n",
        "      dj_dw = dj_dw + (self.l/self.m)*(self.w)\n",
        "      dj_db = np.sum(err)/(self.m) \n",
        "      self.w = self.w-(self.alpha)*dj_dw\n",
        "      self.b = self.b-(self.alpha)*dj_db\n",
        "      J_history.append(self.compute_cost()) \n",
        "      if i%(np.math.ceil((self.iters) / 10)) == 0:\n",
        "        print(\"Iteration :\",i,\" Cost :\",J_history[-1]) \n",
        "    self.J_history = J_history   \n",
        "    return self.w, self.b, J_history\n",
        "\n",
        "  def reset_model(self):\n",
        "    np.random.seed(2)\n",
        "    self.w = 0.01*(np.random.randn(self.n,1)-0.5)\n",
        "    self.b = np.random.randn()\n",
        "    self.J_history = []"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R9h2ogzUGJ5x"
      },
      "source": [
        "##Using our trained model to make a prediction on the test dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vZORot-JGbzW"
      },
      "outputs": [],
      "source": [
        "#Now we load and print our test data\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/linear_test_data.csv\") #Loading data from google drive\n",
        "linear_test_data = np.array(df) #Converting loaded data in a matrix\n",
        "test_id = linear_test_data[:,0] #Getting the Id's of all labels\n",
        "test_id = test_id.reshape(test_id.size,1)\n",
        "X_test = linear_test_data[:,1:21] #Extracting X_test\n",
        "# Printing the first 5 rows of our loaded data\n",
        "print(\"Input:-\\n\",X_test[:5])\n",
        "print(\"Shape of test data:-\",X_test.shape)\n",
        "print(\"Test Id's:-\\n\",test_id[:5])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WD5eWt6a2iH2"
      },
      "source": [
        "Now we make a prediction on the test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nGc9muM72hcj"
      },
      "outputs": [],
      "source": [
        "#Before making a prediction we first have to normalize our data\n",
        "X_train_normalized,mean,std_dev = z_score_normalization(X_train)\n",
        "X_test_normalized = (X_test -mean)/std_dev\n",
        "Y_test_prediction = predict(X_test_normalized,w_ans,b_ans)\n",
        "#We also append the id's of data with our prediction\n",
        "prediction = np.append(test_id,Y_test_prediction,axis = 1)\n",
        "print(prediction[:5])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ukRZPRpQ7ZFO"
      },
      "source": [
        "Now finally we export our prediction back into a csv file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nt_Tvq3N8a16"
      },
      "outputs": [],
      "source": [
        "dfp=pd.DataFrame(prediction) #Convert matrix to dataframe\n",
        "dfp.columns = [\"ids\",\"prediction\"] #Set labels to our dataframe\n",
        "dfp.to_csv(\"linear_test_prediction.csv\",index=False)#Convert dataframe to csv file"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gujj2DIUNv4g"
      },
      "source": [
        "Also for ease of acess we store our trained w and b in npy files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GNm11rZyNveM"
      },
      "outputs": [],
      "source": [
        "#These files can later be loaded using np.load()\n",
        "np.save(\"linear_regression_w.npy\",w_ans)\n",
        "np.save(\"linear_regression_b.npy\",b_ans)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WjQ5X8rePY5N"
      },
      "outputs": [],
      "source": [
        "w_ans = np.load(\"linear_regression_w.npy\")\n",
        "b_ans = np.load(\"linear_regression_b.npy\")\n",
        "print(\"Weights:-\\n\",w_ans)\n",
        "print(\"Bias:-\\n\",b_ans)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iIiU7yTEGdHd"
      },
      "source": [
        "#Polynomial Regression\n",
        "Here we will train a polynomial model using multiple features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eazzrw6F8Kv0"
      },
      "source": [
        "##Creating the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X1raDm_JGhKQ"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/polynomial_train.csv\") #Loading data from google drive\n",
        "polynomial_train_data = np.array(df) #Converting loaded data in a matrix\n",
        "X_train_p = polynomial_train_data[:,1:4]\n",
        "Y_train_p = polynomial_train_data[:,4]\n",
        "Y_train_p = Y_train_p.reshape(Y_train_p.size,1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dFaulWOMbKHv"
      },
      "outputs": [],
      "source": [
        "print(X_train_p[:5])\n",
        "print(Y_train_p[:5])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x5s5PUi9bs46"
      },
      "source": [
        "In this type of regression we have 3 different features A,B and C.We will combine these features to come up with a 3 degree polynomial model for making a prediction.The features are:-\n",
        "a^3 ,b^3 ,c^3 ,(b^2)*c ,b*(c^2) ,(a^2)*b ,(a^2)*c ,a*(b^2) ,a*(c^2) ,a*b*c ,a^2 ,b^2 ,c^2 ,a*b ,a*c ,b*c ,a ,b ,c\n",
        "So we have 19 weights and 1 bias.Now we will restructure our data into a matrix having 19 features (each representing one polynomial term) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4AS7Mra5gqMT"
      },
      "outputs": [],
      "source": [
        "m,n = X_train_p.shape\n",
        "A = X_train_p[:,0].reshape(m,1)\n",
        "B = X_train_p[:,1].reshape(m,1)\n",
        "C = X_train_p[:,2].reshape(m,1)\n",
        "X_train_p_c = np.concatenate((A*A*A,B*B*B,C*C*C,B*B*C,B*C*C,A*A*B,A*A*C,A*B*B,A*C*C,A*B*C,A*A,B*B,C*C,A*B,B*C,C*A,A,B,C), axis=1)\n",
        "print(X_train_p_c[:5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zd_54znhHQNQ"
      },
      "outputs": [],
      "source": [
        "#Now we normalize our data\n",
        "X_train_normalized_p_c,u,s = z_score_normalization(X_train_p_c)\n",
        "print(X_train_normalized_p_c[:5])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p_RTRyHywZJd"
      },
      "source": [
        "##Running Gradient descent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YrlnBPmtKlV8"
      },
      "outputs": [],
      "source": [
        "#Now we can run our function on predefined gradient descent for linear regression\n",
        "np.random.seed(3)\n",
        "w_initial_p = 0.01*(np.random.randn(19,1)-0.5)\n",
        "w_initial_p = w_initial_p.reshape(19,1)\n",
        "b_initial_p = np.random.randn()\n",
        "w_final_p,b_final_p,J_train_history_p,J_cv_history_p = gradient_descent_cv_l(X_train_normalized_p_c,Y_train_p,w_initial_p,b_initial_p,0.1,10000,0.8,0,predict,compute_cost)\n",
        "plt.figure(figsize=(12,4))\n",
        "plt.plot(J_train_history_p , label = \"J train\")\n",
        "plt.plot(J_cv_history_p , label = \"J cv\")\n",
        "plt.title(\"Cost v.s iteration\")\n",
        "plt.ylabel(\"Cost\")\n",
        "plt.xlabel(\"Number of iterations\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wOuFVXjySRWs"
      },
      "source": [
        "##Visualization of the 3 degree Polynomial model\n",
        "We will now plot our output prediction against each of the features A,B,C.Note :- While plotting for A features of B and C will be 0,and so on for B as well as C"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NcwM06WPS8hZ"
      },
      "source": [
        "###Feature A"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0TRqeMv1SPmG"
      },
      "outputs": [],
      "source": [
        "plt.scatter(X_train_normalized_p_c[:,-3],Y_train_p, marker='.',c='r', label = \"Training data\")\n",
        "plt.xlabel(\"Feature A\")\n",
        "plt.ylabel(\"Output\")\n",
        "a = np.array([[1],[0],[0],[0],[0],[0],[0],[0],[0],[0],[1],[0],[0],[0],[0],[0],[1],[0],[0]])\n",
        "f_wb = predict(X_train_normalized_p_c,a*w_final_p,b_final_p)\n",
        "plt.plot(X_train_normalized_p_c[:,-3],f_wb, label = \"Prediction\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "THZZsn39WM1Z"
      },
      "source": [
        "###Feature B"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_WCdY_fFUbC5"
      },
      "outputs": [],
      "source": [
        "plt.scatter(X_train_normalized_p_c[:,-2],Y_train_p, marker='.',c='r',label = \"Training data\")\n",
        "plt.xlabel(\"Feature B\")\n",
        "plt.ylabel(\"Output\")\n",
        "b = np.array([[0],[1],[0],[0],[0],[0],[0],[0],[0],[0],[0],[1],[0],[0],[0],[0],[0],[1],[0]])\n",
        "f_wb = predict(X_train_normalized_p_c,b*w_final_p,b_final_p)\n",
        "plt.plot(X_train_normalized_p_c[:,-2],f_wb, label = \"Prediction\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yzxbEbvmY-Xo"
      },
      "source": [
        "###Feature C"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LZQGGfQeWrlP"
      },
      "outputs": [],
      "source": [
        "plt.scatter(X_train_normalized_p_c[:,-1],Y_train_p, marker='.',c='r' , label = \"Training data\")\n",
        "plt.xlabel(\"Feature C\")\n",
        "plt.ylabel(\"Output\")\n",
        "c = np.array([[0],[0],[1],[0],[0],[0],[0],[0],[0],[0],[0],[0],[1],[0],[0],[0],[0],[0],[1]])\n",
        "f_wb = predict(X_train_normalized_p_c,c*w_final_p,b_final_p)\n",
        "plt.plot(X_train_normalized_p_c[:,-1],f_wb, label = \"Prediction\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T59MoA1Ru87T"
      },
      "source": [
        "##Computing Accuracy of 3 degree polynomial model(Using R_2 score):-"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I8evD1hhu_Y5"
      },
      "outputs": [],
      "source": [
        "r_2 = compute_R_2(w_final_p,b_final_p,X_train_normalized_p_c,Y_train_p,compute_cost)\n",
        "print(\"Accuracy:\",r_2*100,\"%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pio6LJbkw3o7"
      },
      "source": [
        "##Generalizing Polynomial Regression for n features(USING CLASSES)\n",
        "Following code will later be used to make my own python library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rq8T8Zg6xxG1"
      },
      "outputs": [],
      "source": [
        "##Functions defined outside of classes are not to be directly accessible by the users of the library\n",
        "def combo(k,i):\n",
        "  if(k == 1):\n",
        "    return np.array([[i]],dtype=int)\n",
        "  matrix = np.empty((0,k),dtype=int)\n",
        "  for j in range(i+1):\n",
        "    m,n = combo(k-1,j).shape\n",
        "    a = np.full([m,1],i-j,dtype=int)\n",
        "    mt = np.append(a,combo(k-1,j),axis=1)\n",
        "    matrix = np.append(matrix,mt,axis = 0)\n",
        "  return matrix\n",
        "\n",
        "def get_terms(X,d):\n",
        "  m,n = X.shape\n",
        "  e = np.empty((m,0))\n",
        "  for i in range(1,d+1): \n",
        "    c = np.empty((m,0))\n",
        "    mat = combo(n,i)\n",
        "    a,b = mat.shape\n",
        "    for j in range(a):\n",
        "      f = np.prod(np.power(X,mat[j]),axis=1)\n",
        "      f = f.reshape(m,1)\n",
        "      c = np.append(c,f,axis = 1)\n",
        "    e = np.append(e,c,axis=1)\n",
        "  return e\n",
        "\n",
        "def z_score_normalization(X):\n",
        "  m,n = (X).shape\n",
        "  mean = np.sum(X,axis=0)/m \n",
        "  sq_X = X**2 \n",
        "  sq_mean = np.sum(sq_X,axis=0)/m \n",
        "  sq_mean = sq_mean.reshape(1,-1)\n",
        "  std_dev = np.sqrt(sq_mean - (mean)**2)\n",
        "  std_dev = std_dev.reshape(1,-1)\n",
        "  X_normalized = (X - mean)/std_dev  \n",
        "  return X_normalized,mean,std_dev\n",
        "\n",
        "def compute_cost(X,Y,w,b):\n",
        "  m = len(Y) \n",
        "  f_wb = predict(X,w,b)\n",
        "  err = (f_wb - Y)**2\n",
        "  err = err.reshape(m,1)\n",
        "  cost = np.sum(err)\n",
        "  cost = cost/(2*m)\n",
        "  return cost\n",
        "\n",
        "def predict (X,w,b):\n",
        "  m,n = X.shape\n",
        "  a = np.matmul(X,w)\n",
        "  a = a.reshape(m,1)\n",
        "  f_wb = a + b\n",
        "  return f_wb\n",
        "\n",
        "class PolynomialRegression:\n",
        "  def __init__(self,X,degree,Y,alpha,l,iters,ratio):\n",
        "    self.degree = degree\n",
        "    G = get_terms(X,self.degree)\n",
        "    self.X,u,s = z_score_normalization(G)\n",
        "    self.Y = Y\n",
        "    m,n = self.X.shape\n",
        "    self.m = m\n",
        "    self.n = n\n",
        "    self.alpha = alpha\n",
        "    self.l = l\n",
        "    self.iters = iters\n",
        "    self.ratio = ratio\n",
        "    np.random.seed(4)\n",
        "    self.w = 0.01*(np.random.randn(self.n,1)-0.5)\n",
        "    self.b = np.random.randn()\n",
        "    self.J_train_history = []\n",
        "    self.J_cv_history = []\n",
        "  \n",
        "  def predict(self):\n",
        "    a = np.matmul(self.X,self.w)\n",
        "    a = a.reshape(self.m,1)\n",
        "    f_wb = a + self.b\n",
        "    return f_wb\n",
        "  \n",
        "  def compute_cost(self):\n",
        "    f_wb = self.predict()\n",
        "    err = (f_wb - self.Y)**2\n",
        "    err = err.reshape(self.m,1)\n",
        "    cost = np.sum(err) \n",
        "    cost = cost/(2*(self.m))\n",
        "    return cost\n",
        "  \n",
        "  def gradient_descent_cv_l(self):\n",
        "    J_train_history = []\n",
        "    J_cv_history = []\n",
        "    m,n = self.X.shape\n",
        "    a = m*self.ratio\n",
        "    a = round(a)\n",
        "    Z = np.append(self.X,self.Y,axis=1)\n",
        "    np.random.seed(262)\n",
        "    np.random.shuffle(Z)\n",
        "    X_int = Z[:,:-1]\n",
        "    Y_int = Z[:,-1]\n",
        "    Y_int = Y_int.reshape(Y_int.size,1)\n",
        "    X_train = X_int[:a]\n",
        "    X_cv = X_int[a:]\n",
        "    Y_train = Y_int[:a]\n",
        "    Y_cv = Y_int[a:]\n",
        "    m,n = X_train.shape \n",
        "    for i in range(self.iters):\n",
        "      f_wb = predict(X_train,self.w,self.b)\n",
        "      err = f_wb - Y_train\n",
        "      dj_dw = np.matmul(err.T,X_train).T/(self.m)\n",
        "      dj_dw += (self.l/self.m)*(self.w)\n",
        "      dj_db = np.sum(err)/(self.m) \n",
        "      self.w = self.w-(self.alpha)*dj_dw\n",
        "      self.b = self.b-(self.alpha)*dj_db   \n",
        "      J_train_history.append(compute_cost(X_train, Y_train, self.w, self.b)) \n",
        "      J_cv_history.append(compute_cost(X_cv, Y_cv, self.w, self.b))\n",
        "      if i%(np.math.ceil(self.iters / 10)) == 0:\n",
        "        print(\"Iteration :\",i,\" Cost :\",J_train_history[-1])   \n",
        "    self.J_train_history = J_train_history\n",
        "    self.J_cv_history = J_cv_history\n",
        "    return self.w, self.b, J_train_history,J_cv_history \n",
        "  \n",
        "  def reset_model(self):\n",
        "    np.random.seed(4)\n",
        "    self.w = np.random.randn(self.n,1)\n",
        "    self.b = np.random.randn()\n",
        "    self.J_train_history = []\n",
        "    self.J_cv_history = []\n",
        "  \n",
        "  def r2_score(self):\n",
        "    m,n = self.Y.shape\n",
        "    mean = np.sum(self.Y)/m\n",
        "    sq_Y = (self.Y)**2 \n",
        "    sq_mean = np.sum(sq_Y)/m \n",
        "    var_y = sq_mean - (mean)**2 \n",
        "    r_2 = 1 -2*(compute_cost(self.X,self.Y,self.w,self.b))/(var_y)\n",
        "    return r_2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c3HhMXkrHg3X"
      },
      "outputs": [],
      "source": [
        "test2 = PolynomialRegression(X_train_p,5,Y_train_p,0.01,0,100000,0.8)\n",
        "print(test2.predict())\n",
        "print(test2.compute_cost())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O57TCsVG0Num"
      },
      "outputs": [],
      "source": [
        "#Note the parameters w and b, which we receive are of the modified test set(After normalization)\n",
        "test2.reset_model()\n",
        "test2.w,test2.b,test2.J_train_history,test2.J_cv_history = test2.gradient_descent_cv_l()\n",
        "plt.figure(figsize=(12,4))\n",
        "plt.plot(test2.J_train_history, label=\"J train\")\n",
        "plt.plot(test2.J_cv_history, label=\"J cv\" )\n",
        "plt.title(\"Cost v.s iteration\")\n",
        "plt.ylabel(\"Cost\")\n",
        "plt.xlabel(\"Number of iterations\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "izENWwg8RL6G"
      },
      "outputs": [],
      "source": [
        "#We run code for another 10000 iterations\n",
        "test2.w,test2.b,test2.J_train_history,test2.J_cv_history = test2.gradient_descent_cv_l()\n",
        "plt.figure(figsize=(12,4))\n",
        "plt.plot(test2.J_train_history, label=\"J train\")\n",
        "plt.plot(test2.J_cv_history, label=\"J cv\" )\n",
        "plt.title(\"Cost v.s iteration\")\n",
        "plt.ylabel(\"Cost\")\n",
        "plt.xlabel(\"Number of iterations\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6eAJkZKIT7Uw"
      },
      "outputs": [],
      "source": [
        "#We run code for another 10000 iterations\n",
        "test2.w,test2.b,test2.J_train_history,test2.J_cv_history = test2.gradient_descent_cv_l()\n",
        "plt.figure(figsize=(12,4))\n",
        "plt.plot(test2.J_train_history, label=\"J train\")\n",
        "plt.plot(test2.J_cv_history, label=\"J cv\" )\n",
        "plt.title(\"Cost v.s iteration\")\n",
        "plt.ylabel(\"Cost\")\n",
        "plt.xlabel(\"Number of iterations\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3e3i-URsWVGN"
      },
      "outputs": [],
      "source": [
        "#We run code for another 10000 iterations\n",
        "test2.w,test2.b,test2.J_train_history,test2.J_cv_history = test2.gradient_descent_cv_l()\n",
        "plt.figure(figsize=(12,4))\n",
        "plt.plot(test2.J_train_history, label=\"J train\")\n",
        "plt.plot(test2.J_cv_history, label=\"J cv\" )\n",
        "plt.title(\"Cost v.s iteration\")\n",
        "plt.ylabel(\"Cost\")\n",
        "plt.xlabel(\"Number of iterations\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SfaN5cPVdBmY"
      },
      "outputs": [],
      "source": [
        "#We run code for another 10000 iterations\n",
        "test2.w,test2.b,test2.J_train_history,test2.J_cv_history = test2.gradient_descent_cv_l()\n",
        "plt.figure(figsize=(12,4))\n",
        "plt.plot(test2.J_train_history, label=\"J train\")\n",
        "plt.plot(test2.J_cv_history, label=\"J cv\" )\n",
        "plt.title(\"Cost v.s iteration\")\n",
        "plt.ylabel(\"Cost\")\n",
        "plt.xlabel(\"Number of iterations\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "72e7KSi3fn2F"
      },
      "outputs": [],
      "source": [
        "#We run code for another 10000 iterations\n",
        "test2.w,test2.b,test2.J_train_history,test2.J_cv_history = test2.gradient_descent_cv_l()\n",
        "plt.figure(figsize=(12,4))\n",
        "plt.plot(test2.J_train_history, label=\"J train\")\n",
        "plt.plot(test2.J_cv_history, label=\"J cv\" )\n",
        "plt.title(\"Cost v.s iteration\")\n",
        "plt.ylabel(\"Cost\")\n",
        "plt.xlabel(\"Number of iterations\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pliYdazrjFB2"
      },
      "outputs": [],
      "source": [
        "#We run code for another 10000 iterations\n",
        "test2.w,test2.b,test2.J_train_history,test2.J_cv_history = test2.gradient_descent_cv_l()\n",
        "plt.figure(figsize=(12,4))\n",
        "plt.plot(test2.J_train_history, label=\"J train\")\n",
        "plt.plot(test2.J_cv_history, label=\"J cv\" )\n",
        "plt.title(\"Cost v.s iteration\")\n",
        "plt.ylabel(\"Cost\")\n",
        "plt.xlabel(\"Number of iterations\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#We run code for another 10000 iterations\n",
        "test2.w,test2.b,test2.J_train_history,test2.J_cv_history = test2.gradient_descent_cv_l()\n",
        "plt.figure(figsize=(12,4))\n",
        "plt.plot(test2.J_train_history, label=\"J train\")\n",
        "plt.plot(test2.J_cv_history, label=\"J cv\" )\n",
        "plt.title(\"Cost v.s iteration\")\n",
        "plt.ylabel(\"Cost\")\n",
        "plt.xlabel(\"Number of iterations\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "07xk8dFgqqZ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#We run code for another 10000 iterations\n",
        "test2.w,test2.b,test2.J_train_history,test2.J_cv_history = test2.gradient_descent_cv_l()\n",
        "plt.figure(figsize=(12,4))\n",
        "plt.plot(test2.J_train_history, label=\"J train\")\n",
        "plt.plot(test2.J_cv_history, label=\"J cv\" )\n",
        "plt.title(\"Cost v.s iteration\")\n",
        "plt.ylabel(\"Cost\")\n",
        "plt.xlabel(\"Number of iterations\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kUKKaI6U16Bo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#We run code for another 10000 iterations\n",
        "test2.w,test2.b,test2.J_train_history,test2.J_cv_history = test2.gradient_descent_cv_l()\n",
        "plt.figure(figsize=(12,4))\n",
        "plt.plot(test2.J_train_history, label=\"J train\")\n",
        "plt.plot(test2.J_cv_history, label=\"J cv\" )\n",
        "plt.title(\"Cost v.s iteration\")\n",
        "plt.ylabel(\"Cost\")\n",
        "plt.xlabel(\"Number of iterations\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "QCtGB5gB5TCw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vIhNW6S9B3ED"
      },
      "source": [
        "##Computing accuracy of 5 degree polynomial model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1XSvzDFi18Nq"
      },
      "outputs": [],
      "source": [
        "##Checking Accuracy\n",
        "r2 = test2.r2_score()\n",
        "print(\"Accuracy:\",r2*100,\"%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LKmgmbVg-Frd"
      },
      "source": [
        "##Using our trained model to make a prediction on the test dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2MaXiwZwCuk6"
      },
      "outputs": [],
      "source": [
        "#Now we load and print our test data\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/polynomial_test_data.csv\") #Loading data from google drive\n",
        "polynomial_test_data = np.array(df) #Converting loaded data in a matrix\n",
        "test_id_p = polynomial_test_data[:,0] #Getting the Id's of all labels\n",
        "test_id_p = test_id_p.reshape(test_id_p.size,1)\n",
        "X_test_p = polynomial_test_data[:,1:] #Extracting X_test\n",
        "# Printing the first 5 rows of our loaded data\n",
        "print(\"Input:-\\n\",X_test_p[:5])\n",
        "print(\"Shape of test data:-\",X_test_p.shape)\n",
        "print(\"Test Id's:-\\n\",test_id_p[:5])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vFBrRuTEC0Qm"
      },
      "source": [
        "Now we make a prediction on the test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z8uHRLhhC4KL"
      },
      "outputs": [],
      "source": [
        "#Before making a prediction we first have modify and normalize our data\n",
        "G = get_terms(X_train_p,5)\n",
        "X_train_p_normalized,mean,std_dev = z_score_normalization(G)\n",
        "X_test_m_normalized = (get_terms(X_test_p,5)-mean)/std_dev\n",
        "Y_test_prediction_p = predict(X_test_m_normalized,test2.w,test2.b)\n",
        "#We also append the id's of data with our prediction\n",
        "prediction_p = np.append(test_id_p,Y_test_prediction_p,axis=1)\n",
        "print(prediction_p[:5])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61dn5gSXC9HQ"
      },
      "source": [
        "Now finally we export our prediction back into a csv file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jW4UPiyxDJ6o"
      },
      "outputs": [],
      "source": [
        "dfp=pd.DataFrame(prediction_p) #Convert matrix to dataframe\n",
        "dfp.columns = [\"ids\",\"prediction\"] #Set labels to our dataframe\n",
        "dfp.to_csv(\"polynomial_test_prediction.csv\",index=False)#Convert dataframe to csv file"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Saving trained parameters for easier access later"
      ],
      "metadata": {
        "id": "AyhNPn579XrP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ib69rN3k9WF7"
      },
      "outputs": [],
      "source": [
        "#These files can later be loaded using np.load()\n",
        "np.save(\"polynomial_regression_w.npy\",test2.w)\n",
        "np.save(\"polynomial_regression_b.npy\",test2.b)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8drks55l9WF8"
      },
      "outputs": [],
      "source": [
        "#First save the files in the same folder of this notebook\n",
        "w_ans = np.load(\"polynomial_regression_w.npy\")\n",
        "b_ans = np.load(\"polynomial_regression_b.npy\")\n",
        "print(\"Weights:-\\n\",w_ans)\n",
        "print(\"Bias:-\\n\",b_ans)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ruqFGgqXVut3"
      },
      "source": [
        "#Visualizing the data for classification tasks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "96AFIzz5_uDS"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/classification_train.csv\") #Loading data from google drive\n",
        "classification_train_data = np.array(df) #Converting loaded data in a matrix\n",
        "X_train_c = classification_train_data[:,2:786]\n",
        "Y_train_c = classification_train_data[:,1]\n",
        "Y_train_c = Y_train_c.reshape(Y_train_c.size,1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CC8LVDCZAvta"
      },
      "outputs": [],
      "source": [
        "print(X_train_c[:5])\n",
        "print(Y_train_c[:5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Uw8JrKJeWbb"
      },
      "outputs": [],
      "source": [
        "#Here we write code to display some of the images with their labels\n",
        "for i in range(9):\n",
        "  data = X_train_c[i].reshape(28,28)\n",
        "  plt.imshow(data,cmap=\"Greys_r\")\n",
        "  plt.title(f\"{Y_train_c[i]}\")\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8kA9d1xe_oNC"
      },
      "source": [
        "# KNN\n",
        "(K nearest neigbours algorithm)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8GElLHN2jLoE"
      },
      "source": [
        "##Creating the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mSL6DLPR3K8H"
      },
      "outputs": [],
      "source": [
        "def predict_knn(X,Y,k,p):\n",
        "  a,b = X.shape\n",
        "  m,n = p.shape #Get number of predictions to be made and number of features\n",
        "  f = len(np.unique(Y))\n",
        "  a = np.sum((p)**2,axis=1).reshape(-1,1)\n",
        "  a = a - 2*np.matmul(p,X.T)\n",
        "  a = a + np.sum((X.T)**2,axis=0).reshape(1,-1)\n",
        "  a = np.argsort(a,axis=1)\n",
        "  a = Y[a].reshape(-1,Y.size)  \n",
        "  c = a[:,:k]\n",
        "  c = pd.DataFrame(c)\n",
        "  d = np.array(c.mode(axis=1,numeric_only=True))\n",
        "  d = d[:,0]\n",
        "  ans = np.array(d,dtype=int).reshape(d.size,1)\n",
        "  return ans"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KOfNRG50zS1A"
      },
      "outputs": [],
      "source": [
        "p = X_train_c[:5]\n",
        "predict_knn(X_train_c,Y_train_c,3,p)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y8BWX77o1D3G"
      },
      "source": [
        "##Visualizing KNN data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vBz2Wr0c1ogy"
      },
      "outputs": [],
      "source": [
        "#We will pick the first 2 features of our training dataset and plot it on the 2D plane(After normalization)\n",
        "X_plot = X_train_c[:,:2]\n",
        "X_plot_0 = []\n",
        "X_plot_1 = []\n",
        "X_plot_2 = []\n",
        "X_plot_3 = []\n",
        "X_plot_4 = []\n",
        "X_plot_5 = []\n",
        "X_plot_6 = []\n",
        "X_plot_7 = []\n",
        "X_plot_8 = []\n",
        "X_plot_9 = []\n",
        "Y_plot = Y_train_c\n",
        "for i in range(Y_train_c.size):\n",
        "  if Y_plot[i] == 0:\n",
        "    X_plot_0.append(X_plot[i])\n",
        "  elif Y_plot[i] == 1:\n",
        "    X_plot_1.append(X_plot[i])\n",
        "  elif Y_plot[i] == 2:\n",
        "    X_plot_2.append(X_plot[i])\n",
        "  elif Y_plot[i] == 3:\n",
        "    X_plot_3.append(X_plot[i])\n",
        "  elif Y_plot[i] == 4:\n",
        "    X_plot_4.append(X_plot[i])\n",
        "  elif Y_plot[i] == 5:\n",
        "    X_plot_5.append(X_plot[i])\n",
        "  elif Y_plot[i] == 6:\n",
        "    X_plot_6.append(X_plot[i])\n",
        "  elif Y_plot[i] == 7:\n",
        "    X_plot_7.append(X_plot[i])\n",
        "  elif Y_plot[i] == 8:\n",
        "    X_plot_8.append(X_plot[i])\n",
        "  elif Y_plot[i] == 9:\n",
        "    X_plot_9.append(X_plot[i])\n",
        "X_0 = np.array(X_plot_0)\n",
        "X_1 = np.array(X_plot_1)\n",
        "X_2 = np.array(X_plot_2)\n",
        "X_3 = np.array(X_plot_3)\n",
        "X_4 = np.array(X_plot_4)\n",
        "X_5 = np.array(X_plot_5)\n",
        "X_6 = np.array(X_plot_6)\n",
        "X_7 = np.array(X_plot_7)\n",
        "X_8 = np.array(X_plot_8)\n",
        "X_9 = np.array(X_plot_9)\n",
        "plt.scatter(X_0[:,0],X_0[:,1],color='k',marker='o',label=0)\n",
        "plt.scatter(X_1[:,0],X_1[:,1],color='b',marker='o',label=1)\n",
        "plt.scatter(X_2[:,0],X_2[:,1],color='g',marker='o',label=2)\n",
        "plt.scatter(X_3[:,0],X_3[:,1],color='r',marker='o',label=3)\n",
        "plt.scatter(X_4[:,0],X_4[:,1],color='c',marker='o',label=4)\n",
        "plt.scatter(X_5[:,0],X_5[:,1],color='m',marker='o',label=5)\n",
        "plt.scatter(X_6[:,0],X_6[:,1],color='y',marker='o',label=6)\n",
        "plt.scatter(X_7[:,0],X_7[:,1],color='#3d251e',marker='o',label=7)\n",
        "plt.scatter(X_8[:,0],X_8[:,1],color='#ed7014',marker='o',label=8)\n",
        "plt.scatter(X_9[:,0],X_9[:,1],color='#8f00ff',marker='o',label=9)\n",
        "plt.title(\"Labels plotted against the 1st two pixel grayscale values\")\n",
        "plt.xlabel(\"First pixel\")\n",
        "plt.ylabel(\"Second pixel\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VtW8FM3vC5PZ"
      },
      "source": [
        "##Computing accuracy of KNN model\n",
        "For computing accuracy we first need to split our training dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EWsppLK7C-lr"
      },
      "outputs": [],
      "source": [
        "X_train_c_t,X_cv_c,Y_train_c_t,Y_cv_c = train_test_split(X_train_c,Y_train_c,0.999)\n",
        "print(X_train_c_t.shape)\n",
        "print(X_cv_c.shape)\n",
        "print(Y_train_c_t.shape)\n",
        "print(Y_cv_c.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Di8LtLN4RE0_"
      },
      "outputs": [],
      "source": [
        "def accuracy_knn(X_train,X_cv,Y_train,Y_cv,k):\n",
        "  cnt = 0\n",
        "  m,n = X_cv.shape\n",
        "  cnt = (predict_knn(X_train,Y_train,k,X_cv) == Y_cv).sum()\n",
        "  return cnt/m"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7V1loGcvqYpD"
      },
      "source": [
        "##Plotting K v.s Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "urNS4xKzUSpP"
      },
      "outputs": [],
      "source": [
        "#Takes time(approx 2 minutes)\n",
        "acc = []\n",
        "for i in range(1,25):\n",
        "  acc.append(accuracy_knn(X_train_c_t,X_cv_c,Y_train_c_t,Y_cv_c,i))\n",
        "plt.figure(figsize=(12,4))\n",
        "plt.plot(acc)\n",
        "plt.title(\"Accuracy v.s k\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.xlabel(\"k\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rXfm_Nv4x5Qu"
      },
      "source": [
        "Most ideal value of k seems to be 4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LTxtuRVJup7U"
      },
      "source": [
        "##Generalizing knn using classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2DffoEyeu3o2"
      },
      "outputs": [],
      "source": [
        "def train_test_split(X,Y,ratio):\n",
        "    #ratio is the ratio of the train set to that of complete set\n",
        "    m,n = X.shape\n",
        "    a = m*ratio\n",
        "    a = round(a)\n",
        "    Z = np.append(X,Y,axis=1)\n",
        "    np.random.seed(261)\n",
        "    np.random.shuffle(Z)\n",
        "    X = Z[:,:-1]\n",
        "    Y = Z[:,-1]\n",
        "    Y = Y.reshape(Y.size,1)\n",
        "    X_train_r = X[:a]\n",
        "    X_cv_r = X[a:]\n",
        "    Y_train_r = Y[:a]\n",
        "    Y_cv_r = Y[a:]\n",
        "    return X_train_r,X_cv_r,Y_train_r,Y_cv_r\n",
        "\n",
        "def predict_knn(X,Y,k,p):\n",
        "   a,b = X.shape\n",
        "   m,n = p.shape #Get number of predictions to be made and number of features\n",
        "   f = len(np.unique(Y))\n",
        "   a = np.sum((p)**2,axis=1).reshape(-1,1)\n",
        "   a = a - 2*np.matmul(p,X.T)\n",
        "   a = a + np.sum((X.T)**2,axis=0).reshape(1,-1)\n",
        "   a = np.argsort(a,axis=1)\n",
        "   a = Y[a].reshape(-1,Y.size)  \n",
        "   c = a[:,:k]\n",
        "   c = pd.DataFrame(c)\n",
        "   d = np.array(c.mode(axis=1,numeric_only=True))\n",
        "   d = d[:,0]\n",
        "   ans = np.array(d,dtype=int).reshape(d.size,1)\n",
        "   return ans\n",
        "\n",
        "class knn:\n",
        "  def __init__(self,X,Y,k,p,ratio):\n",
        "    self.X = X\n",
        "    self.Y = Y\n",
        "    self.k = k\n",
        "    self.p = p\n",
        "    self.ratio = ratio\n",
        "    self.f = len(np.unique(self.Y))\n",
        "  \n",
        "  def predict_knn(self):\n",
        "   a,b = self.X.shape\n",
        "   m,n = self.p.shape #Get number of predictions to be made and number of features\n",
        "   f = len(np.unique(self.Y))\n",
        "   a = np.sum((self.p)**2,axis=1).reshape(-1,1)\n",
        "   a = a - 2*np.matmul(self.p,self.X.T)\n",
        "   a = a + np.sum((self.X.T)**2,axis=0).reshape(1,-1)\n",
        "   a = np.argsort(a,axis=1)\n",
        "   a = self.Y[a].reshape(-1,self.Y.size)  \n",
        "   c = a[:,:self.k]\n",
        "   c = pd.DataFrame(c)\n",
        "   d = np.array(c.mode(axis=1,numeric_only=True))\n",
        "   d = d[:,0]\n",
        "   ans = np.array(d,dtype=int).reshape(d.size,1)\n",
        "   return ans\n",
        "\n",
        "  def accuracy_knn(self):\n",
        "    X_train_c_t,X_cv_c_t,Y_train_c_t,Y_cv_c_t = train_test_split(self.X,self.Y,self.ratio)\n",
        "    cnt = 0\n",
        "    m,n = X_cv_c_t.shape\n",
        "    cnt = (predict_knn(X_train_c_t,Y_train_c_t,self.k,X_cv_c_t) == Y_cv_c_t).sum()\n",
        "    return cnt/m"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FPEkbQrpJc2P"
      },
      "source": [
        "##Visualizing performance of knn model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jbtdh9jjJbcv"
      },
      "outputs": [],
      "source": [
        "for i in range(9):\n",
        "  data = X_train_c[i].reshape(28,28)\n",
        "  plt.imshow(data,cmap=\"Greys_r\")\n",
        "  Y_knn = predict_knn(X_train_c,Y_train_c,3,X_train_c[i].reshape(1,-1))\n",
        "  plt.title(f\"label:{Y_train_c[i]} knn:{Y_knn}\")\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GJKxS516x0aM"
      },
      "source": [
        "##Making a prediction on test dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MuSPFq9Ax_hT"
      },
      "outputs": [],
      "source": [
        "#Now we load and print our test data\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/classification_test.csv\") #Loading data from google drive\n",
        "classification_test_data = np.array(df) #Converting loaded data in a matrix\n",
        "test_id_c = classification_test_data[:,0] #Getting the Id's of all labels\n",
        "test_id_c = test_id_c.reshape(test_id_c.size,1)\n",
        "X_test_c = classification_test_data[:,1:] #Extracting X_test\n",
        "# Printing the first 5 rows of our loaded data\n",
        "print(\"Input:-\\n\",X_test_c[:5])\n",
        "print(\"Shape of test data:-\",X_test_c.shape)\n",
        "print(\"Test Id's:-\\n\",test_id_c[:5])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qulnBnqsjlgl"
      },
      "source": [
        "Now we make a prediction on the test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ydm9O-HfyJq"
      },
      "outputs": [],
      "source": [
        "#Code takes time approx 15 minutes\n",
        "Y_test_prediction_c = predict_knn(X_train_c,Y_train_c,3,X_test_c)\n",
        "print(Y_test_prediction_c)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ask7mLU5s8HS"
      },
      "outputs": [],
      "source": [
        "#We also append the id's of data with our prediction\n",
        "prediction_c = np.append(test_id_c,Y_test_prediction_c,axis=1)\n",
        "print(prediction_c[:5])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z6VV5_L8jlgn"
      },
      "source": [
        "Now finally we export our prediction back into a csv file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "shxyFWvTjlgn"
      },
      "outputs": [],
      "source": [
        "dfc=pd.DataFrame(prediction_c) #Convert matrix to dataframe\n",
        "dfc.columns = [\"ids\",\"prediction\"] #Set labels to our dataframe\n",
        "dfc.to_csv(\"classification_test_prediction_knn.csv\",index=False)#Convert dataframe to csv file"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k2eZiNB--K2M"
      },
      "source": [
        "#Logistic Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s37VdTQNbOkQ"
      },
      "source": [
        "##Creating the model\n",
        "We are going to create a softmax model for making a prediction on multiple classes(SOFTMAX)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "57pOJPgiYpWg"
      },
      "source": [
        "###Initializing model parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wbKCXrNkYu6M"
      },
      "outputs": [],
      "source": [
        "np.random.seed(5)\n",
        "#For initializing small values near 0\n",
        "#As there are 10 different classes and 784 different features\n",
        "w_initial_lo = 0.0001*(np.random.randn(10,784)-0.5)\n",
        "b_initial_lo = np.random.randn(1,10)\n",
        "print(w_initial_lo)\n",
        "print(b_initial_lo)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oxrhMrtEHTA4"
      },
      "source": [
        "###Modifying the data\n",
        "Before running gradient descent we will apply one hot encoding to Y_train_c and we will generate a vector containing frequency of every class(Useful for gradient descent),After one-hot encoding we will perform other operations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mevqWZpjWeKm"
      },
      "outputs": [],
      "source": [
        "def modify_data_lo(X,Y):\n",
        "  #Getting the frequency array\n",
        "  Y_train_ca = np.bincount(Y.flatten())\n",
        "  Y_train_ca = Y_train_ca.reshape(1,Y_train_ca.size)\n",
        "  #One hot encoding the data (Run this cell after just after loading classification data)\n",
        "  ab = pd.DataFrame(Y,columns = [\"Category\"])\n",
        "  Y_train_cb = pd.get_dummies(ab,columns = [\"Category\"])\n",
        "  Y_train_cb = np.array(Y_train_cb)\n",
        "  return Y_train_cb,Y_train_ca"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qap6pMPvYgXn"
      },
      "outputs": [],
      "source": [
        "Y_train_cw,Y_train_cb = modify_data_lo(X_train_c,Y_train_c)\n",
        "print(\"For updating w:-\\n\",Y_train_cw)\n",
        "print(\"For updating b:-\\n\",Y_train_cb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F3_Zpo8KZIIF"
      },
      "source": [
        "##Predict Function\n",
        "We define 2 predict functions one for probability and other for actual prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LAY2U0UWZNg6"
      },
      "outputs": [],
      "source": [
        "def predict_lo (X,w,b):\n",
        "  \"\"\"\n",
        "  Arguments\n",
        "  X(2D numpy array of training examples) Shape(m,n)\n",
        "  w(numpy array of model parameters) Shape(n,1)\n",
        "  b(model parameter) scalar \n",
        "  \"\"\"\n",
        "  m,n = X.shape\n",
        "  a = np.matmul(X,w.T) + b\n",
        "  #Applying softmax activation\n",
        "  c = np.exp(a)\n",
        "  d = np.sum(c,axis=1).reshape(m,1)\n",
        "  probability = c/d\n",
        "  prediction = probability.argmax(axis=1).reshape(m,1)\n",
        "  return probability,prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RRMVzk5JXNGM"
      },
      "outputs": [],
      "source": [
        "probability,prediction = predict_lo(X_train_c,w_initial_lo,b_initial_lo)\n",
        "print(probability)\n",
        "print(prediction)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eNJEGY2zaUJk"
      },
      "source": [
        "##Computing Cost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ZekxcKSaaZq"
      },
      "outputs": [],
      "source": [
        "def compute_cost_lo(X,Y,w,b):\n",
        "  m = len(Y) #Getting the number of training examples\n",
        "  probability,prediction = predict_lo(X,w,b)\n",
        "  l = np.choose(Y.T,probability.T).T\n",
        "  loss = (-1)*np.log(l)\n",
        "  cost = np.sum(loss)/m\n",
        "  return cost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9-e_exkNbeRN"
      },
      "outputs": [],
      "source": [
        "print(compute_cost_lo(X_train_c,Y_train_c,w_initial_lo,b_initial_lo))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "az4na_ekdeiH"
      },
      "source": [
        "##Computing cost(with regularization)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VLJmROildNi7"
      },
      "outputs": [],
      "source": [
        "def compute_cost_lo_l(X,Y,w,b,l):\n",
        "  m = len(Y)\n",
        "  cost = compute_cost_lo(X,Y,w,b)\n",
        "  cost += (l/2*m)*np.sum(w**2) #Adding regularized term\n",
        "  return cost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JkxH8PgNeHtD"
      },
      "outputs": [],
      "source": [
        "print(compute_cost_lo_l(X_train_c,Y_train_c,w_initial_lo,b_initial_lo,0.01))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oHJwNQ62eyO9"
      },
      "source": [
        "##Running Gradient descent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "psdkJPp0VNJI"
      },
      "outputs": [],
      "source": [
        "def gradient_descent_lo(X,Y,w_in,b_in,alpha,iters,ratio,l,predict,compute_cost_lo,modify_data_lo):\n",
        "  \n",
        "  J_train_history = [] #We create a list containg cost after every iteration(For later plotting and analysis)\n",
        "  J_cv_history = []\n",
        "  w = w_in.copy() \n",
        "  b = b_in\n",
        "  \n",
        "  X_train,X_cv,Y_train,Y_cv = train_test_split(X,Y,ratio)\n",
        "  m,n = X_train.shape #Getting the number of training examples\n",
        "  Y_train = Y_train.astype(np.int64)\n",
        "  Y_cv = Y_cv.astype(np.int64)\n",
        "  X_w,X_b = modify_data_lo(X_train,Y_train)\n",
        "  for i in range(iters):\n",
        "    #Compute gradient dj_dw and dj_db\n",
        "    probability,prediction = predict_lo(X_train,w,b)\n",
        "    dj_db = (np.sum(probability,axis=0).reshape(1,-1) - X_b)/m #Here -1 in .reshape() automatically gets the possible dimension(According to the number of elements in the array)\n",
        "    dj_dw = (l*w - np.matmul((X_w - probability).T,X_train))/m\n",
        "    #Update parameters w,b (Simultaneously)\n",
        "    w = w-alpha*dj_dw\n",
        "    b = b-alpha*dj_db\n",
        "    #Record Cost J after every iteration\n",
        "    # Print cost after interval of 10 times,[-1] refers to the last element(reverse indexing)\n",
        "    if i%(np.math.ceil(iters / 10)) == 0:\n",
        "     #Note:- Even though we have applied regularization the method for computing training and testing error is kept same\n",
        "      J_train_history.append(compute_cost_lo(X_train, Y_train, w, b)) #Add the current cost to J_history\n",
        "      J_cv_history.append(compute_cost_lo(X_cv, Y_cv, w, b))\n",
        "      print(\"Iteration :\",i,\" Cost :\",J_train_history[-1])  #F-strings used to encode variables i and J_history  \n",
        "  return w, b, J_train_history,J_cv_history #return final w,b and J_history(for graphing)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XsKnByI5Jdii"
      },
      "outputs": [],
      "source": [
        "#Running logistic gradient descent\n",
        "w_final_lo,b_final_lo,J_train_history,J_cv_history = gradient_descent_lo(X_train_c,Y_train_c,w_initial_lo,b_initial_lo,0.00001,1000,0.8,0,predict_lo,compute_cost_lo,modify_data_lo)\n",
        "plt.figure(figsize=(12,4))\n",
        "plt.plot(J_train_history , label = \"J train\")\n",
        "plt.plot(J_cv_history , label = \"J cv\")\n",
        "plt.title(\"Cost v.s iteration\")\n",
        "plt.ylabel(\"Cost\")\n",
        "plt.xlabel(\"Number of iterations\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JPqQqHQK0zaX"
      },
      "source": [
        "As we see our cost function fluctuates so we apply z-score normalization on the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ddPKbzro4MLZ"
      },
      "outputs": [],
      "source": [
        "X_train_cn,u,s = z_score_normalization(X_train_c)\n",
        "w_final_lo,b_final_lo,J_train_history,J_cv_history = gradient_descent_lo(X_train_cn,Y_train_c,w_initial_lo,b_initial_lo,0.1,1000,0.8,0,predict_lo,compute_cost_lo,modify_data_lo)\n",
        "plt.figure(figsize=(12,4))\n",
        "plt.plot(J_train_history , label = \"J train\")\n",
        "plt.plot(J_cv_history , label = \"J cv\")\n",
        "plt.title(\"Cost v.s iteration\")\n",
        "plt.ylabel(\"Cost\")\n",
        "plt.xlabel(\"Number of iterations\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9cdVmZyZD8lo"
      },
      "outputs": [],
      "source": [
        "#Further continuing the descent with  learning rate\n",
        "w_final_lo,b_final_lo,J_train_history,J_cv_history = gradient_descent_lo(X_train_cn,Y_train_c,w_final_lo,b_final_lo,0.1,1000,0.8,0,predict_lo,compute_cost_lo,modify_data_lo)\n",
        "plt.figure(figsize=(12,4))\n",
        "plt.plot(J_train_history , label = \"J train\")\n",
        "plt.plot(J_cv_history , label = \"J cv\")\n",
        "plt.title(\"Cost v.s iteration\")\n",
        "plt.ylabel(\"Cost\")\n",
        "plt.xlabel(\"Number of iterations\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8sxTjEMxdV0o"
      },
      "source": [
        "The cost function has almost saturated now we compute accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0lJsMiRt8m82"
      },
      "source": [
        "##Computing Accuracy of classification model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LQA3cTjtgXZG"
      },
      "outputs": [],
      "source": [
        "def accuracy_lo(X,Y,w,b):\n",
        "  m = len(Y) #Getting number of training examples\n",
        "  probability,prediction = predict_lo(X,w,b)\n",
        "  cnt = (Y == prediction).sum()\n",
        "  return cnt/m"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UnJNeWUx9YPP"
      },
      "outputs": [],
      "source": [
        "a = accuracy_lo(X_train_cn,Y_train_c,w_final_lo,b_final_lo)\n",
        "print(f\"Accuracy:- {a*100}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Be42OpR0l9DA"
      },
      "source": [
        "##Plotting the prediction\n",
        "Here we used our trained model to make prediction on the training dataset(So it can be compared with the previous plot of the classification data in knn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1S3-JR8jjYp2"
      },
      "outputs": [],
      "source": [
        "#We will pick the first 2 features of our training dataset and plot it on the 2D plane(After normalization)\n",
        "X_plot = X_train_c[:,:2]\n",
        "X_plot_0 = []\n",
        "X_plot_1 = []\n",
        "X_plot_2 = []\n",
        "X_plot_3 = []\n",
        "X_plot_4 = []\n",
        "X_plot_5 = []\n",
        "X_plot_6 = []\n",
        "X_plot_7 = []\n",
        "X_plot_8 = []\n",
        "X_plot_9 = []\n",
        "probability,Y_plot = predict_lo(X_train_cn,w_final_lo,b_initial_lo)\n",
        "for i in range(Y_train_c.size):\n",
        "  if Y_plot[i] == 0:\n",
        "    X_plot_0.append(X_plot[i])\n",
        "  elif Y_plot[i] == 1:\n",
        "    X_plot_1.append(X_plot[i])\n",
        "  elif Y_plot[i] == 2:\n",
        "    X_plot_2.append(X_plot[i])\n",
        "  elif Y_plot[i] == 3:\n",
        "    X_plot_3.append(X_plot[i])\n",
        "  elif Y_plot[i] == 4:\n",
        "    X_plot_4.append(X_plot[i])\n",
        "  elif Y_plot[i] == 5:\n",
        "    X_plot_5.append(X_plot[i])\n",
        "  elif Y_plot[i] == 6:\n",
        "    X_plot_6.append(X_plot[i])\n",
        "  elif Y_plot[i] == 7:\n",
        "    X_plot_7.append(X_plot[i])\n",
        "  elif Y_plot[i] == 8:\n",
        "    X_plot_8.append(X_plot[i])\n",
        "  elif Y_plot[i] == 9:\n",
        "    X_plot_9.append(X_plot[i])\n",
        "X_0 = np.array(X_plot_0)\n",
        "X_1 = np.array(X_plot_1)\n",
        "X_2 = np.array(X_plot_2)\n",
        "X_3 = np.array(X_plot_3)\n",
        "X_4 = np.array(X_plot_4)\n",
        "X_5 = np.array(X_plot_5)\n",
        "X_6 = np.array(X_plot_6)\n",
        "X_7 = np.array(X_plot_7)\n",
        "X_8 = np.array(X_plot_8)\n",
        "X_9 = np.array(X_plot_9)\n",
        "plt.scatter(X_0[:,0],X_0[:,1],color='k',marker='o',label=0)\n",
        "plt.scatter(X_1[:,0],X_1[:,1],color='b',marker='o',label=1)\n",
        "plt.scatter(X_2[:,0],X_2[:,1],color='g',marker='o',label=2)\n",
        "plt.scatter(X_3[:,0],X_3[:,1],color='r',marker='o',label=3)\n",
        "plt.scatter(X_4[:,0],X_4[:,1],color='c',marker='o',label=4)\n",
        "plt.scatter(X_5[:,0],X_5[:,1],color='m',marker='o',label=5)\n",
        "plt.scatter(X_6[:,0],X_6[:,1],color='y',marker='o',label=6)\n",
        "plt.scatter(X_7[:,0],X_7[:,1],color='#3d251e',marker='o',label=7)\n",
        "plt.scatter(X_8[:,0],X_8[:,1],color='#ed7014',marker='o',label=8)\n",
        "plt.scatter(X_9[:,0],X_9[:,1],color='#8f00ff',marker='o',label=9)\n",
        "plt.title(\"Labels plotted against the 1st two pixel grayscale values\")\n",
        "plt.xlabel(\"First pixel\")\n",
        "plt.ylabel(\"Second pixel\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wU5tSGBDL3kP"
      },
      "source": [
        "##Visualizing performance of logistic model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GV1wbbSoLvUr"
      },
      "outputs": [],
      "source": [
        "for i in range(9):\n",
        "  data = X_train_c[i].reshape(28,28)\n",
        "  plt.imshow(data,cmap=\"Greys_r\")\n",
        "  prob,Y_lo = predict_lo(X_train_cn,w_final_lo,b_final_lo)\n",
        "  ans = Y_lo[i]\n",
        "  plt.title(f\"label:{Y_train_c[i]} logistic:{ans}\")\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dTdG3wjOCCOk"
      },
      "source": [
        "##Generalizing Logistic Regression for n features(USING CLASSES)\n",
        "Following code will later be used to make my own python library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q7XSdAV1CUrL"
      },
      "outputs": [],
      "source": [
        "##Functions defined outside of classes are not to be directly accessible by the users of the library\n",
        "def predict_lo(X,w,b):\n",
        "  m,n = X.shape\n",
        "  a = np.matmul(X,w.T) + b\n",
        "  c = np.exp(a)\n",
        "  d = np.sum(c,axis=1).reshape(m,1)\n",
        "  probability = c/d\n",
        "  prediction = probability.argmax(axis=1).reshape(m,1)\n",
        "  return probability,prediction\n",
        "\n",
        "def compute_cost_lo(X,Y,w,b):\n",
        "  m = len(Y)\n",
        "  probability,prediction = predict_lo(X,w,b)\n",
        "  l = np.choose(Y.T,probability.T).T\n",
        "  loss = (-1)*np.log(l)\n",
        "  cost = np.sum(loss)/m\n",
        "  return cost\n",
        "\n",
        "def train_test_split(X,Y,ratio):\n",
        "    m,n = X.shape\n",
        "    a = m*ratio\n",
        "    a = round(a)\n",
        "    X_train_r = X[:a]\n",
        "    X_cv_r = X[a:]\n",
        "    Y_train_r = Y[:a]\n",
        "    Y_cv_r = Y[a:]\n",
        "    return X_train_r,X_cv_r,Y_train_r,Y_cv_r\n",
        "\n",
        "def modify_data_lo(X,Y):\n",
        "  Y_train_ca = np.bincount(Y.flatten())\n",
        "  Y_train_ca = Y_train_ca.reshape(1,Y_train_ca.size)\n",
        "  ab = pd.DataFrame(Y,columns = [\"Category\"])\n",
        "  Y_train_cb = pd.get_dummies(ab,columns = [\"Category\"])\n",
        "  Y_train_cb = np.array(Y_train_cb)\n",
        "  return Y_train_cb,Y_train_ca\n",
        "\n",
        "class LogisticRegression:\n",
        "  def __init__(self,X,Y,alpha,l,iters,ratio):\n",
        "    self.X,self.mean,self.std_dev = z_score_normalization(X)\n",
        "    self.Y = Y\n",
        "    m,n = self.X.shape\n",
        "    self.m = m\n",
        "    self.n = n\n",
        "    self.k = len(np.unique(self.Y))\n",
        "    self.alpha = alpha\n",
        "    self.l = l\n",
        "    self.iters = iters\n",
        "    self.ratio = ratio\n",
        "    np.random.seed(7)\n",
        "    self.w = 0.0001*(np.random.randn(self.k,self.n)-0.5)\n",
        "    self.b = np.random.randn(1,self.k)\n",
        "    self.J_train_history = []\n",
        "    self.J_cv_history = []\n",
        "  \n",
        "  def predict(self):  \n",
        "    a = np.matmul(self.X,(self.w).T) + self.b\n",
        "    c = np.exp(a)\n",
        "    d = np.sum(c,axis=1).reshape(self.m,1)\n",
        "    probability = c/d\n",
        "    prediction = probability.argmax(axis=1).reshape(self.m,1)\n",
        "    return probability,prediction\n",
        "  \n",
        "  def compute_cost(self):\n",
        "    probability,prediction = self.predict()\n",
        "    l = np.choose((self.Y).T,probability.T).T\n",
        "    loss = (-1)*np.log(l)\n",
        "    cost = np.sum(loss)/(self.m)\n",
        "    return cost\n",
        "  \n",
        "  def gradient_descent(self):\n",
        "    J_train_history = []\n",
        "    J_cv_history = []\n",
        "    X_train,X_cv,Y_train,Y_cv = train_test_split(self.X,self.Y,self.ratio)\n",
        "    m,n = X_train.shape \n",
        "    Y_train = Y_train.astype(np.int64)\n",
        "    Y_cv = Y_cv.astype(np.int64)\n",
        "    X_w,X_b = modify_data_lo(X_train,Y_train)\n",
        "    for i in range(self.iters):\n",
        "      probability,prediction = predict_lo(X_train,self.w,self.b)\n",
        "      dj_db = (np.sum(probability,axis=0).reshape(1,-1) - X_b)/m \n",
        "      dj_dw = ((self.l)*(self.w) - np.matmul((X_w - probability).T,X_train))/m\n",
        "      self.w = self.w-self.alpha*dj_dw\n",
        "      self.b = self.b-self.alpha*dj_db\n",
        "      if i<=100000: \n",
        "        if i%(np.math.ceil(self.iters/10)) == 0:\n",
        "          J_train_history.append(compute_cost_lo(X_train, Y_train, self.w, self.b))\n",
        "          J_cv_history.append(compute_cost_lo(X_cv, Y_cv, self.w, self.b))\n",
        "          print(f\"Iteration :{i} Cost :{J_train_history[-1]}\")   \n",
        "    self.J_train_history = J_train_history\n",
        "    self.J_cv_history = J_cv_history\n",
        "    return self.w, self.b, J_train_history,J_cv_history \n",
        "  \n",
        "  def reset_model(self):\n",
        "    np.random.seed(7)\n",
        "    self.w = 0.0001*(np.random.randn(self.k,self.n)-0.5)\n",
        "    self.b = np.random.randn(1,self.k)\n",
        "    self.J_train_history = []\n",
        "    self.J_cv_history = []\n",
        "  \n",
        "  def accuracy(self):\n",
        "    probability,prediction = self.predict()\n",
        "    cnt = (self.Y == prediction).sum()\n",
        "    return cnt/(self.m)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mYTjRxRKAyaj"
      },
      "source": [
        "##Using our trained data to make a prediction on test dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UWnugfDFXY5k"
      },
      "outputs": [],
      "source": [
        "#Now we load and print our test data\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/classification_test.csv\") #Loading data from google drive\n",
        "classification_test_data = np.array(df) #Converting loaded data in a matrix\n",
        "test_id_c = classification_test_data[:,0] #Getting the Id's of all labels\n",
        "test_id_c = test_id_c.reshape(test_id_c.size,1)\n",
        "X_test_c = classification_test_data[:,1:] #Extracting X_test\n",
        "# Printing the first 5 rows of our loaded data\n",
        "print(\"Input:-\\n\",X_test_c[:5])\n",
        "print(\"Shape of test data:-\",X_test_c.shape)\n",
        "print(\"Test Id's:-\\n\",test_id_c[:5])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZeJcE1eXY5k"
      },
      "source": [
        "Now we make a prediction on the test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uux-3TppXY5k"
      },
      "outputs": [],
      "source": [
        "#We normalize data before making prediction\n",
        "Xn,u,s = z_score_normalization(X_test_c)\n",
        "X_test_cn = (X_test_c - u)/s\n",
        "probability,Y_test_prediction_c = predict_lo(X_test_cn,w_final_lo,b_final_lo)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-UtlKl3mXY5l"
      },
      "outputs": [],
      "source": [
        "#We also append the id's of data with our prediction\n",
        "prediction_c = np.append(test_id_c,Y_test_prediction_c,axis=1)\n",
        "print(prediction_c[:5])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eA7p6RfDXY5l"
      },
      "source": [
        "Now finally we export our prediction back into a csv file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j7BylZ89XY5l"
      },
      "outputs": [],
      "source": [
        "dfc=pd.DataFrame(prediction_c) #Convert matrix to dataframe\n",
        "dfc.columns = [\"ids\",\"prediction\"] #Set labels to our dataframe\n",
        "dfc.to_csv(\"classification_test_prediction_logistic.csv\",index=False)#Convert dataframe to csv file"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Saving trained parameters for easier access later"
      ],
      "metadata": {
        "id": "Sd0I3FooTnqS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wfnr3qWNTnqS"
      },
      "outputs": [],
      "source": [
        "#These files can later be loaded using np.load()\n",
        "np.save(\"logistic_w.npy\",w_final_lo)\n",
        "np.save(\"logistic_b.npy\",b_final_lo)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KX5be39kTnqS"
      },
      "outputs": [],
      "source": [
        "w_ans = np.load(\"logistic_w.npy\")\n",
        "b_ans = np.load(\"logistic_b.npy\")\n",
        "print(\"Weights:-\\n\",w_ans)\n",
        "print(\"Bias:-\\n\",b_ans)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pk6s5EuY_xVe"
      },
      "source": [
        "# Neural Networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nfjklvZs_6MD"
      },
      "source": [
        "##Creating the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a0drk_qhMZgl"
      },
      "outputs": [],
      "source": [
        "#For the L'th layer W is a matrix where each column corresponds to the weights of each neuron and b is the bias vector(with just 1 row)\n",
        "def dense(A_in,W,b,activation,z_list):\n",
        "  z = np.matmul(A_in,W) + b\n",
        "  z_list.append(z)\n",
        "  if activation == \"sigmoid\":\n",
        "    A_out = 1/(1+np.exp(-z)) \n",
        "  elif activation == \"linear\":\n",
        "    A_out = z\n",
        "  elif activation == \"relu\":\n",
        "    A_out = np.maximum(z,0)\n",
        "  elif activation == \"tanh\":\n",
        "    A_out = (np.exp(z) - np.exp(-z))/(np.exp(z) + np.exp(-z))\n",
        "  elif activation == \"softmax\":\n",
        "    a = np.exp(z)\n",
        "    sum = np.sum(a,axis=1).reshape(-1,1)\n",
        "    A_out = a/sum\n",
        "  return A_out,z_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DekWSM6oSTmY"
      },
      "outputs": [],
      "source": [
        "#Now we will constuct the gen_w_b function which will define the architecture of our neural network(Unilayer)\n",
        "def gen_w_b(X,units):\n",
        "  W_list = []\n",
        "  b_list = []\n",
        "  m,n = X.shape #m is no.of training examples\n",
        "  np.random.seed(264)\n",
        "  for i in range(len(units)):\n",
        "    if i==0:\n",
        "      W_list.append(0.0000001*(np.random.randn(n,units[0])-0.5))\n",
        "    else:\n",
        "      W_list.append(0.0000001*(np.random.randn(units[i-1],units[i])-0.5))\n",
        "    b_list.append(0.0000001*(np.random.randn(1,units[i])-0.5))\n",
        "  return W_list,b_list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qO7yMTSFMIGh"
      },
      "source": [
        "##Forward propogation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0OXtdCd-v1fv"
      },
      "outputs": [],
      "source": [
        "#Now this function will perform forward propogation in the network\n",
        "def f_prop(X,W_list,b_list,activation):\n",
        "  a_out = X\n",
        "  z_list = []\n",
        "  z_list.append(X)\n",
        "  a_out_l = []\n",
        "  a_out_l.append(X)\n",
        "  for i in range(len(activation)):\n",
        "    a_out,z_list = dense(a_out,W_list[i],b_list[i],activation[i],z_list)\n",
        "    a_out_l.append(a_out)\n",
        "  return a_out,z_list,a_out_l"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qHmZKInsMAcU"
      },
      "source": [
        "##Cost function\n",
        "The cost function of the neural network will depend on the type of problem we are solving.As the activation of the last neuron also depends on the type of problem we are solving"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "40H6B5izIH_U"
      },
      "outputs": [],
      "source": [
        "def compute_cost(X,Y,W_list,b_list,activation,cost):\n",
        "   m = len(Y) #Getting the number of training examples\n",
        "   if cost == \"mean_squared_error\": #Regression problems\n",
        "      f_wb,z_list,a_out_l = f_prop(X,W_list,b_list,activation)\n",
        "      err = (f_wb - Y)**2\n",
        "      err = err.reshape(m,1)\n",
        "      cost = np.sum(err)/(2*m)\n",
        "   elif cost == \"binary_cross_entropy\":  #Binary class classification\n",
        "     f_wb,z_list,a_out_l = f_prop(X,W_list,b_list,activation)\n",
        "     loss = -Y*np.log(f_wb)-(1-Y)*np.log(1-f_wb)\n",
        "     cost = np.sum(loss)/m\n",
        "   elif cost == \"cross_entropy\": #Multi class classification\n",
        "      probability,z_list,a_out_l = f_prop(X,W_list,b_list,activation)\n",
        "      Y = np.array(Y,dtype=\"int64\")\n",
        "      l = np.choose(Y.T,probability.T).T\n",
        "      loss = (-1)*np.log(l)\n",
        "      cost = np.sum(loss)/m\n",
        "   return cost"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BKazTVUNTCpz"
      },
      "source": [
        "##Regularized cost function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dc0W7Ik6TH3Q"
      },
      "outputs": [],
      "source": [
        "def compute_cost_l(X,Y,W_list,b_list,activation,cost,l):\n",
        "   m = len(Y) #Getting the number of training examples\n",
        "   if cost == \"mean_squared_error\": #Regression problems\n",
        "      f_wb,z_list,a_out_l = f_prop(X,W_list,b_list,activation)\n",
        "      err = (f_wb - Y)**2\n",
        "      err = err.reshape(m,1)\n",
        "      cost = np.sum(err)/(2*m)\n",
        "   elif cost == \"binary_cross_entropy\":  #Binary class classification\n",
        "     f_wb,z_list,a_out_l = f_prop(X,W_list,b_list,activation)\n",
        "     loss = -Y*np.log(f_wb)-(1-Y)*np.log(1-f_wb)\n",
        "     cost = np.sum(loss)/m\n",
        "   elif cost == \"cross_entropy\": #Multi class classification\n",
        "      probability,z_list,a_out_l = f_prop(X,W_list,b_list,activation)\n",
        "      l = np.choose(Y.T,probability.T).T\n",
        "      loss = (-1)*np.log(l)\n",
        "      cost = np.sum(loss)/m\n",
        "   cnt = 0\n",
        "   for i in range(len(W_list)):\n",
        "     cnt+=np.sum((W_list[i])**2)\n",
        "   cnt = (l/2*m)*cnt\n",
        "   cost+=cnt\n",
        "   return cost"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8iOrOLLOgPXR"
      },
      "source": [
        "## Back Propogation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b075Qxf7m9IP"
      },
      "outputs": [],
      "source": [
        "#Now we compute the function which has derivatives of all activation functions\n",
        "def func_d(z,activation):\n",
        "  if activation == \"sigmoid\":\n",
        "    A_out = np.exp(-z)/(1+np.exp(-z))**2 \n",
        "  elif activation == \"linear\":\n",
        "    A_out = np.full(z.shape,1)\n",
        "  elif activation == \"relu\":\n",
        "    def d_relu(z):\n",
        "      if z>0:\n",
        "        return 1\n",
        "      else:\n",
        "        return 0\n",
        "    d_relu = np.vectorize(d_relu)\n",
        "    A_out = d_relu(z)\n",
        "  elif activation == \"tanh\":\n",
        "    a_out = (np.exp(z) - np.exp(-z))/(np.exp(z) + np.exp(-z))\n",
        "    A_out = 1 - (a_out)**2\n",
        "  elif activation == \"softmax\":\n",
        "    a = np.exp(z)\n",
        "    sum = np.sum(a,axis=1).reshape(-1,1)\n",
        "    a_out = a/sum\n",
        "    A_out = a_out - (a_out)**2\n",
        "  return A_out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kxQ5mpoj2wQT"
      },
      "source": [
        "###Assumptions\n",
        "For the neural network to work correctly it has been assumed that for the cost function \"Binary cross entropy\" and \"Cross entropy\" the activation of the output layer is \"sigmoid\" and \"softmax\" respectively."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nJW1VsRGgabW"
      },
      "outputs": [],
      "source": [
        "def b_prop(X,Y,W_list,b_list,activation,cost,l,a_out_l,z_list):\n",
        "  m,o = X.shape\n",
        "  n = len(activation)\n",
        "  dj_dw = []\n",
        "  dj_db = []\n",
        "  if cost == \"mean_squared_error\":\n",
        "    act = activation[-1]\n",
        "    Y_s = Y\n",
        "  elif cost == \"cross_entropy\":\n",
        "    act = \"linear\"\n",
        "    ab = pd.DataFrame(Y,columns = [\"Category\"])\n",
        "    Y_s = pd.get_dummies(ab,columns = [\"Category\"])\n",
        "    Y_s = np.array(Y_s)\n",
        "  else:\n",
        "    act = \"linear\"\n",
        "    Y_s = Y\n",
        "  if n==1:\n",
        "    s = (a_out_l[-1]-Y_s)/m\n",
        "    s = s*func_d(z_list[-1],act)\n",
        "    dj_dw_i = np.matmul((a_out_l[-2]).T,s)\n",
        "    dj_db_i = np.sum(s,axis=0).reshape(1,-1)\n",
        "    dj_dw_i += (l/m)*W_list[-1]\n",
        "    dj_dw.append(dj_dw_i)\n",
        "    dj_db.append(dj_db_i)\n",
        "  else:\n",
        "    s = (a_out_l[-1]-Y_s)/m\n",
        "    s = s*func_d(z_list[-1],act)\n",
        "    dj_dw_i = np.matmul((a_out_l[-2]).T,s)\n",
        "    dj_db_i = np.sum(s,axis=0).reshape(1,-1)\n",
        "    dj_dw_i += (l/m)*W_list[-1]\n",
        "    dj_dw.append(dj_dw_i)\n",
        "    dj_db.append(dj_db_i)\n",
        "    for i in range(2,n+1):\n",
        "      s = np.matmul(s,(W_list[1-i]).T)\n",
        "      s = s*func_d(z_list[-i],activation[-i])\n",
        "      dj_dw_i = np.matmul((a_out_l[-(i+1)]).T,s)\n",
        "      dj_db_i = np.sum(s,axis=0).reshape(1,-1)\n",
        "      dj_dw_i += (l/m)*W_list[-i]\n",
        "      dj_dw.append(dj_dw_i)\n",
        "      dj_db.append(dj_db_i)\n",
        "  dj_dw.reverse()\n",
        "  dj_db.reverse()\n",
        "  return dj_dw,dj_db"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bez7zroQLl02"
      },
      "source": [
        "##Gradient Descent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BkzAC1sDnwh1"
      },
      "outputs": [],
      "source": [
        "def gradient_descent_nn(X,Y,W_list,b_list,alpha,iters,ratio,l,cost,activation):\n",
        "  J_train_history = [] #We create a list containg cost after every iteration(For later plotting and analysis)\n",
        "  J_cv_history = []\n",
        "  W_list = W_list.copy()\n",
        "  b_list = b_list\n",
        "  X_train,X_cv,Y_train,Y_cv = train_test_split(X,Y,ratio)\n",
        "  m,n = X_train.shape #Getting the number of training examples\n",
        "  for i in range(iters):\n",
        "    #Compute gradient dj_dw and dj_db\n",
        "    f_wb,z_list,a_out_l = f_prop(X_train,W_list,b_list,activation)\n",
        "    dj_dw,dj_db = b_prop(X_train,Y_train,W_list,b_list,activation,cost,l,a_out_l,z_list)\n",
        "    #Update parameters W,b (Simultaneously)\n",
        "    for j in range(len(W_list)):\n",
        "      W_list[j] = W_list[j] - alpha*(dj_dw[j])\n",
        "      b_list[j] = b_list[j] - alpha*(dj_db[j])\n",
        "    #Record Cost J after every iteration\n",
        "    #Note:- Even though we have applied regularization the method for computing training and testing error is still same\n",
        "    J_train_history.append(compute_cost(X_train,Y_train,W_list,b_list,activation,cost)) #Add the current cost to J_history\n",
        "    J_cv_history.append(compute_cost(X_cv,Y_cv,W_list,b_list,activation,cost))\n",
        "    # Print cost after interval of 10 times,[-1] refers to the last element(reverse indexing)\n",
        "    if i%(np.math.ceil(iters / 10)) == 0:\n",
        "      print(\"Iteration :\",i,\" Cost :\",J_train_history[-1]) #F-strings used to encode variables i and J_history  \n",
        "  return W_list, b_list, J_train_history,J_cv_history #return final w,b and J_history(for graphing)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-lWoKHcYfn8d"
      },
      "source": [
        "##Unilayer regression by neural network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "esJENSfUgBNb"
      },
      "source": [
        "###Neural network trained on linear train data(single layer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ozjIgKNHhXy4"
      },
      "outputs": [],
      "source": [
        "#Now we load and print our training data\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/linear_train.csv\") #Loading data from google drive\n",
        "linear_train_data = np.array(df) #Converting loaded data in a matrix\n",
        "X_train_l = linear_train_data[:,1:21] #Extracting X_train(Input data)\n",
        "Y_train_l = linear_train_data[:,21] ##Extracting Y_train(Labels)\n",
        "Y_train_l = Y_train_l.reshape(Y_train_l.size,1)\n",
        "# Printing the first 5 rows of our loaded data\n",
        "print(\"Input\\n\",X_train_l[:5])\n",
        "print(\"labels\\n\",Y_train_l[:5])\n",
        "print(X_train_l.shape)\n",
        "print(Y_train_l.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QxyyUYXujzhu"
      },
      "outputs": [],
      "source": [
        "#Normalizing the data\n",
        "X_train_ln,u,s = z_score_normalization(X_train_l)\n",
        "print(\"Normalized Data:-\",X_train_ln[:5])\n",
        "print(\"Mean of orignal data:-\",u)\n",
        "print(\"std_dev of orignal data:-\",s)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rI26dGDEgAtB"
      },
      "outputs": [],
      "source": [
        "units_lin = [20,1]\n",
        "activation_lin = [\"relu\",\"linear\"]\n",
        "W_lin,b_lin = gen_w_b(X_train_ln,units_lin)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fV76C5j4huoW"
      },
      "outputs": [],
      "source": [
        "#Running gradient descent for neural network\n",
        "W_lin,b_lin,J_train_history,J_cv_history = gradient_descent_nn(X_train_ln,Y_train_l,W_lin,b_lin,0.001,1000,0.8,0,\"mean_squared_error\",activation_lin)\n",
        "plt.figure(figsize=(12,4))\n",
        "plt.plot(J_train_history , label = \"J train\")\n",
        "plt.plot(J_cv_history , label = \"J cv\")\n",
        "plt.title(\"Cost v.s iteration\")\n",
        "plt.ylabel(\"Cost\")\n",
        "plt.xlabel(\"Number of iterations\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ZvpziXigxVT"
      },
      "source": [
        "###Neural network trained on polynomial train data(single layer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oSM0q0jeg3ZX"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/polynomial_train.csv\") #Loading data from google drive\n",
        "polynomial_train_data = np.array(df) #Converting loaded data in a matrix\n",
        "X_train_p = polynomial_train_data[:,1:4]\n",
        "Y_train_p = polynomial_train_data[:,4]\n",
        "Y_train_p = Y_train_p.reshape(Y_train_p.size,1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X7Cl0WUnBiPG"
      },
      "outputs": [],
      "source": [
        "print(X_train_p[:5])\n",
        "print(Y_train_p[:5])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def combo(k,i):\n",
        "  if(k == 1):\n",
        "    return np.array([[i]],dtype=int)\n",
        "  matrix = np.empty((0,k),dtype=int)\n",
        "  for j in range(i+1):\n",
        "    m,n = combo(k-1,j).shape\n",
        "    a = np.full([m,1],i-j,dtype=int)\n",
        "    mt = np.append(a,combo(k-1,j),axis=1)\n",
        "    matrix = np.append(matrix,mt,axis = 0)\n",
        "  return matrix\n",
        "\n",
        "def get_terms(X,d):\n",
        "  m,n = X.shape\n",
        "  e = np.empty((m,0))\n",
        "  for i in range(1,d+1): \n",
        "    c = np.empty((m,0))\n",
        "    mat = combo(n,i)\n",
        "    a,b = mat.shape\n",
        "    for j in range(a):\n",
        "      f = np.prod(np.power(X,mat[j]),axis=1)\n",
        "      f = f.reshape(m,1)\n",
        "      c = np.append(c,f,axis = 1)\n",
        "    e = np.append(e,c,axis=1)\n",
        "  return e"
      ],
      "metadata": {
        "id": "pjYBSElmXpGu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_AZpsqb9Buae"
      },
      "outputs": [],
      "source": [
        "#Getting the terms for a 3 degree polynomial\n",
        "X_train_pm = get_terms(X_train_p,5)\n",
        "print(X_train_pm[:5])\n",
        "print(X_train_pm.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gwY0eOGLCLTJ"
      },
      "outputs": [],
      "source": [
        "#Normalizing the data\n",
        "X_train_pmn,u,s = z_score_normalization(X_train_pm)\n",
        "print(\"Normalized Data:-\",X_train_pmn[:5])\n",
        "print(\"Mean of orignal data:-\",u)\n",
        "print(\"std_dev of orignal data:-\",s)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AfXHHgCpCUYH"
      },
      "outputs": [],
      "source": [
        "units_poly = [20,1]\n",
        "activation_poly = [\"relu\",\"linear\"]\n",
        "W_poly,b_poly = gen_w_b(X_train_pmn,units_poly)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PmpRgvLlCaXl"
      },
      "outputs": [],
      "source": [
        "#Takes a lot of time\n",
        "W_poly,b_poly,J_train_history,J_cv_history = gradient_descent_nn(X_train_pmn,Y_train_p,W_poly,b_poly,0.0000001,1000,0.8,0,\"mean_squared_error\",activation_poly)\n",
        "plt.figure(figsize=(12,4))\n",
        "plt.plot(J_train_history , label = \"J train\")\n",
        "plt.plot(J_cv_history , label = \"J cv\")\n",
        "plt.title(\"Cost v.s iteration\")\n",
        "plt.ylabel(\"Cost\")\n",
        "plt.xlabel(\"Number of iterations\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wHcuBxWpXxh1"
      },
      "source": [
        "###Neural network trained on classification data(single layer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TcyTYctqX7ds"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/classification_train.csv\") #Loading data from google drive\n",
        "classification_train_data = np.array(df) #Converting loaded data in a matrix\n",
        "X_train_c = classification_train_data[:,2:786]\n",
        "Y_train_c = classification_train_data[:,1]\n",
        "Y_train_c = Y_train_c.reshape(Y_train_c.size,1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HIpogKWJYAfz"
      },
      "outputs": [],
      "source": [
        "print(X_train_c[:5])\n",
        "print(Y_train_c[:5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O8H5HImLYKhR"
      },
      "outputs": [],
      "source": [
        "#Normalizing the data\n",
        "X_train_cn,u,s = z_score_normalization(X_train_c)\n",
        "print(\"Normalized Data:-\",X_train_cn[:5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LCMuAbqIYad3"
      },
      "outputs": [],
      "source": [
        "units_cl = [100,10]\n",
        "activation_cl = [\"relu\",\"softmax\"]\n",
        "W_cl,b_cl = gen_w_b(X_train_cn,units_cl)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sVr3iIX0Y0kV"
      },
      "outputs": [],
      "source": [
        "W_cl,b_cl,J_train_history,J_cv_history = gradient_descent_nn(X_train_cn,Y_train_c,W_cl,b_cl,0.1,1000,0.8,0,\"cross_entropy\",activation_cl)\n",
        "plt.figure(figsize=(12,4))\n",
        "plt.plot(J_train_history , label = \"J train\")\n",
        "plt.plot(J_cv_history , label = \"J cv\")\n",
        "plt.title(\"Cost v.s iteration\")\n",
        "plt.ylabel(\"Cost\")\n",
        "plt.xlabel(\"Number of iterations\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NJ436QqSspcE"
      },
      "source": [
        "##Genralizing neural network for n layers using classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-d6hfMdqamZK"
      },
      "outputs": [],
      "source": [
        "def z_score_normalization(X):\n",
        "  m,n = (X).shape\n",
        "  mean = np.sum(X,axis=0)/m \n",
        "  mean = mean.reshape(1,-1)\n",
        "  sq_X = X**2 \n",
        "  sq_mean = np.sum(sq_X,axis=0)/m \n",
        "  sq_mean = sq_mean.reshape(1,-1)\n",
        "  std_dev = np.sqrt(sq_mean - (mean)**2) \n",
        "  std_dev = std_dev.reshape(1,-1)\n",
        "  X_normalized = (X - mean)/std_dev \n",
        "  return X_normalized, mean, std_dev\n",
        "\n",
        "def train_test_split(X,Y,ratio):\n",
        "    m,n = X.shape\n",
        "    a = m*ratio\n",
        "    a = round(a)\n",
        "    Z = np.append(X,Y,axis=1)\n",
        "    np.random.seed(261)\n",
        "    np.random.shuffle(Z)\n",
        "    X = Z[:,:-1]\n",
        "    Y = Z[:,-1]\n",
        "    Y = Y.reshape(Y.size,1)\n",
        "    X_train_r = X[:a]\n",
        "    X_cv_r = X[a:]\n",
        "    Y_train_r = Y[:a]\n",
        "    Y_cv_r = Y[a:]\n",
        "    return X_train_r,X_cv_r,Y_train_r,Y_cv_r\n",
        "\n",
        "def dense(A_in,W,b,activation,z_list):\n",
        "  z = np.matmul(A_in,W) + b\n",
        "  z_list.append(z)\n",
        "  if activation == \"sigmoid\":\n",
        "    A_out = 1/(1+np.exp(-z)) \n",
        "  elif activation == \"linear\":\n",
        "    A_out = z\n",
        "  elif activation == \"relu\":\n",
        "    A_out = np.maximum(z,0)\n",
        "  elif activation == \"tanh\":\n",
        "    A_out = (np.exp(z) - np.exp(-z))/(np.exp(z) + np.exp(-z))\n",
        "  elif activation == \"softmax\":\n",
        "    a = np.exp(z)\n",
        "    sum = np.sum(a,axis=1).reshape(-1,1)\n",
        "    A_out = a/sum\n",
        "  return A_out,z_list\n",
        "\n",
        "def gen_w_b(X,units):\n",
        "  W_list = []\n",
        "  b_list = []\n",
        "  m,n = X.shape \n",
        "  np.random.seed(111)\n",
        "  for i in range(len(units)):\n",
        "    if i==0:\n",
        "      W_list.append(1*(np.random.randn(n,units[0])-0.5))\n",
        "    else:\n",
        "      W_list.append(1*(np.random.randn(units[i-1],units[i])-0.5))\n",
        "    b_list.append(1*(np.random.randn(1,units[i])-0.5))\n",
        "  return W_list,b_list\n",
        "\n",
        "def f_prop(X,W_list,b_list,activation):\n",
        "  a_out = X\n",
        "  z_list = []\n",
        "  z_list.append(X)\n",
        "  a_out_l = []\n",
        "  a_out_l.append(X)\n",
        "  for i in range(len(activation)):\n",
        "    a_out,z_list = dense(a_out,W_list[i],b_list[i],activation[i],z_list)\n",
        "    a_out_l.append(a_out)\n",
        "  return a_out,z_list,a_out_l\n",
        "\n",
        "def compute_cost(X,Y,W_list,b_list,activation,cost):\n",
        "   m = len(Y)\n",
        "   if cost == \"mean_squared_error\": \n",
        "      f_wb,z_list,a_out_l = f_prop(X,W_list,b_list,activation)\n",
        "      err = (f_wb - Y)**2\n",
        "      err = err.reshape(m,1)\n",
        "      cost = np.sum(err)/(2*m)\n",
        "   elif cost == \"binary_cross_entropy\":  \n",
        "     f_wb,z_list,a_out_l = f_prop(X,W_list,b_list,activation)\n",
        "     loss = -Y*np.log(f_wb)-(1-Y)*np.log(1-f_wb)\n",
        "     cost = np.sum(loss)/m\n",
        "   elif cost == \"cross_entropy\": \n",
        "      probability,z_list,a_out_l = f_prop(X,W_list,b_list,activation)\n",
        "      Y = np.array(Y,dtype=\"int64\")\n",
        "      l = np.choose(Y.T,probability.T).T\n",
        "      loss = (-1)*np.log(l)\n",
        "      cost = np.sum(loss)/m\n",
        "   return cost\n",
        "\n",
        "def func_d(z,activation):\n",
        "  if activation == \"sigmoid\":\n",
        "    A_out = np.exp(-z)/(1+np.exp(-z))**2 \n",
        "  elif activation == \"linear\":\n",
        "    A_out = np.full(z.shape,1)\n",
        "  elif activation == \"relu\":\n",
        "    def d_relu(z):\n",
        "      if z>0:\n",
        "        return 1\n",
        "      else:\n",
        "        return 0\n",
        "    d_relu = np.vectorize(d_relu)\n",
        "    A_out = d_relu(z)\n",
        "  elif activation == \"tanh\":\n",
        "    a_out = (np.exp(z) - np.exp(-z))/(np.exp(z) + np.exp(-z))\n",
        "    A_out = 1 - (a_out)**2\n",
        "  elif activation == \"softmax\":\n",
        "    a = np.exp(z)\n",
        "    sum = np.sum(a,axis=1).reshape(-1,1)\n",
        "    a_out = a/sum\n",
        "    A_out = a_out - (a_out)**2\n",
        "  return A_out\n",
        "\n",
        "def b_prop(X,Y,W_list,b_list,activation,cost,l,a_out_l,z_list):\n",
        "  m,o = X.shape\n",
        "  n = len(activation)\n",
        "  dj_dw = []\n",
        "  dj_db = []\n",
        "  if cost == \"mean_squared_error\":\n",
        "    act = activation[-1]\n",
        "    Y_s = Y\n",
        "  elif cost == \"cross_entropy\":\n",
        "    act = \"linear\"\n",
        "    ab = pd.DataFrame(Y,columns = [\"Category\"])\n",
        "    Y_s = pd.get_dummies(ab,columns = [\"Category\"])\n",
        "    Y_s = np.array(Y_s)\n",
        "  else:\n",
        "    act = \"linear\"\n",
        "    Y_s = Y\n",
        "  if n==1:\n",
        "    s = (a_out_l[-1]-Y_s)/m\n",
        "    s = s*func_d(z_list[-1],act)\n",
        "    dj_dw_i = np.matmul((a_out_l[-2]).T,s)\n",
        "    dj_db_i = np.sum(s,axis=0).reshape(1,-1)\n",
        "    dj_dw_i += (l/m)*W_list[-1]\n",
        "    dj_dw.append(dj_dw_i)\n",
        "    dj_db.append(dj_db_i)\n",
        "  else:\n",
        "    s = (a_out_l[-1]-Y_s)/m\n",
        "    s = s*func_d(z_list[-1],act)\n",
        "    dj_dw_i = np.matmul((a_out_l[-2]).T,s)\n",
        "    dj_db_i = np.sum(s,axis=0).reshape(1,-1)\n",
        "    dj_dw_i += (l/m)*W_list[-1]\n",
        "    dj_dw.append(dj_dw_i)\n",
        "    dj_db.append(dj_db_i)\n",
        "    for i in range(2,n+1):\n",
        "      s = np.matmul(s,(W_list[1-i]).T)\n",
        "      s = s*func_d(z_list[-i],activation[-i])\n",
        "      dj_dw_i = np.matmul((a_out_l[-(i+1)]).T,s)\n",
        "      dj_db_i = np.sum(s,axis=0).reshape(1,-1)\n",
        "      dj_dw_i += (l/m)*W_list[-i]\n",
        "      dj_dw.append(dj_dw_i)\n",
        "      dj_db.append(dj_db_i)\n",
        "  dj_dw.reverse()\n",
        "  dj_db.reverse()\n",
        "  return dj_dw,dj_db\n",
        "\n",
        "def gradient_descent_nn(X,Y,W_list,b_list,alpha,iters,ratio,l,cost,activation):\n",
        "  J_train_history = [] \n",
        "  J_cv_history = []\n",
        "  W_list = W_list.copy()\n",
        "  b_list = b_list\n",
        "  X_train,X_cv,Y_train,Y_cv = train_test_split(X,Y,ratio)\n",
        "  m,n = X_train.shape \n",
        "  for i in range(iters):\n",
        "    f_wb,z_list,a_out_l = f_prop(X_train,W_list,b_list,activation)\n",
        "    dj_dw,dj_db = b_prop(X_train,Y_train,W_list,b_list,activation,cost,l,a_out_l,z_list)\n",
        "    for j in range(len(W_list)):\n",
        "      W_list[j] = W_list[j] - alpha*(dj_dw[j])\n",
        "      b_list[j] = b_list[j] - alpha*(dj_db[j])\n",
        "    J_train_history.append(compute_cost(X_train,Y_train,W_list,b_list,activation,cost)) \n",
        "    J_cv_history.append(compute_cost(X_cv,Y_cv,W_list,b_list,activation,cost))\n",
        "    if i%(np.math.ceil(iters / 10)) == 0:\n",
        "      print(\"Iteration :\",i,\" Cost :\",J_train_history[-1])   \n",
        "  return W_list, b_list, J_train_history,J_cv_history \n",
        "\n",
        "class NeuralNetwork:\n",
        "  def __init__(self,X,Y,units,activation,cost,alpha,iters,ratio,l):\n",
        "    self.X,u,s = z_score_normalization(X)\n",
        "    self.Y = Y\n",
        "    self.units = units\n",
        "    self.activation = activation\n",
        "    self.cost = cost\n",
        "    self.w,self.b = gen_w_b(self.X,self.units)\n",
        "    self.alpha = alpha\n",
        "    self.iters = iters\n",
        "    self.ratio = ratio\n",
        "    self.l = l\n",
        "    self.J_train_history = []\n",
        "    self.J_cv_history = []\n",
        "  \n",
        "  def predict(self):\n",
        "    a_out,g1,g2 = f_prop(self.X,self.w,self.b,self.activation)\n",
        "    if self.cost == \"binary_cross_entropy\":\n",
        "      def d_s(z):\n",
        "        if z>=0.5:\n",
        "          return 1\n",
        "        else:\n",
        "          return 0\n",
        "      d_s = np.vectorize(d_s)\n",
        "      A_out = d_s(a_out)\n",
        "    elif self.cost == \"cross_entropy\":\n",
        "      m,n = a_out.shape\n",
        "      A_out = a_out.argmax(axis=1).reshape(m,1)\n",
        "    else:\n",
        "      A_out=a_out\n",
        "    return A_out\n",
        "  \n",
        "  def compute_cost(self):\n",
        "    return compute_cost(self.X,self.Y,self.w,self.b,self.activation,self.cost)\n",
        "  \n",
        "  def gradient_descent(self):\n",
        "    self.w,self.b,self.J_train_history,self.J_cv_history = gradient_descent_nn(self.X,self.Y,self.w,self.b,self.alpha,self.iters,self.ratio,self.l,self.cost,self.activation)\n",
        "    return self.w,self.b,self.J_train_history,self.J_cv_history\n",
        "\n",
        "  def accuracy(self):\n",
        "    if self.cost == \"binary_cross_entropy\" or self.cost == \"cross_entropy\":\n",
        "      m,n = self.Y.shape\n",
        "      cnt = (self.predict() == self.Y).sum()\n",
        "      acc = cnt/(m)\n",
        "    else:\n",
        "      m,n = self.Y.shape\n",
        "      mean = np.sum(self.Y)/m\n",
        "      sq_Y = (self.Y)**2 \n",
        "      sq_mean = np.sum(sq_Y)/m \n",
        "      var_y = sq_mean - (mean)**2 \n",
        "      acc = 1 -2*(self.compute_cost())/(var_y) \n",
        "    return acc\n",
        "\n",
        "  def reset_model(self):\n",
        "    self.w,self.b = gen_w_b(self.X,self.units)\n",
        "    self.J_cv_history = []\n",
        "    self.J_cv_history = []"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SJJ9xY-pNBxy"
      },
      "source": [
        "###Regression neural network for linear data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HTAt-hIhSg5h"
      },
      "outputs": [],
      "source": [
        "m,n = X_train_l.shape\n",
        "print(0.8*m)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DvZ3PhdVNBFa"
      },
      "outputs": [],
      "source": [
        "#Defining the network\n",
        "nn_l = NeuralNetwork(X_train_l,Y_train_l,[20,10,5,1],[\"relu\",\"relu\",\"relu\",\"linear\"],\"mean_squared_error\",0.0001,1000,0.8,0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TL0CIst0O68q"
      },
      "outputs": [],
      "source": [
        "#Prediction\n",
        "print(nn_l.predict())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cW75soHcPerv"
      },
      "outputs": [],
      "source": [
        "#Computing cost\n",
        "print(nn_l.compute_cost())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YrssWaXiRC20"
      },
      "outputs": [],
      "source": [
        "#Running gradient descent\n",
        "nn_l.reset_model()\n",
        "nn_l.w,nn_l.b,J_train_history,J_cv_history = nn_l.gradient_descent()\n",
        "plt.figure(figsize=(12,4))\n",
        "plt.plot(J_train_history , label = \"J train\")\n",
        "plt.plot(J_cv_history , label = \"J cv\")\n",
        "plt.title(\"Cost v.s iteration\")\n",
        "plt.ylabel(\"Cost\")\n",
        "plt.xlabel(\"Number of iterations\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mfTcbMe1KE9R"
      },
      "outputs": [],
      "source": [
        "#Running for another 1000 iterations\n",
        "nn_l.w,nn_l.b,J_train_history,J_cv_history = nn_l.gradient_descent()\n",
        "plt.figure(figsize=(12,4))\n",
        "plt.plot(J_train_history , label = \"J train\")\n",
        "plt.plot(J_cv_history , label = \"J cv\")\n",
        "plt.title(\"Cost v.s iteration\")\n",
        "plt.ylabel(\"Cost\")\n",
        "plt.xlabel(\"Number of iterations\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RMHF6ds5bPzw"
      },
      "outputs": [],
      "source": [
        "#Running for another 500 iterations\n",
        "nn_l.iters = 500\n",
        "nn_l.w,nn_l.b,J_train_history,J_cv_history = nn_l.gradient_descent()\n",
        "plt.figure(figsize=(12,4))\n",
        "plt.plot(J_train_history , label = \"J train\")\n",
        "plt.plot(J_cv_history , label = \"J cv\")\n",
        "plt.title(\"Cost v.s iteration\")\n",
        "plt.ylabel(\"Cost\")\n",
        "plt.xlabel(\"Number of iterations\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Computing accuracy"
      ],
      "metadata": {
        "id": "VzHMOIY6K0fd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JFnFagY0QrXm"
      },
      "outputs": [],
      "source": [
        "#Computing accuracy\n",
        "print(\"Accuracy:\",nn_l.accuracy()*100,\"%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TJzxR8sYV3e7"
      },
      "source": [
        "###Making a prediction on test dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aUnEGu08sPc1"
      },
      "outputs": [],
      "source": [
        "#Now we load and print our test data\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/linear_test_data.csv\") #Loading data from google drive\n",
        "linear_test_data = np.array(df) #Converting loaded data in a matrix\n",
        "test_id = linear_test_data[:,0] #Getting the Id's of all labels\n",
        "test_id = test_id.reshape(test_id.size,1)\n",
        "X_test = linear_test_data[:,1:21] #Extracting X_test\n",
        "# Printing the first 5 rows of our loaded data\n",
        "print(\"Input:-\\n\",X_test[:5])\n",
        "print(\"Shape of test data:-\",X_test.shape)\n",
        "print(\"Test Id's:-\\n\",test_id[:5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5yZhIdorr1L7"
      },
      "outputs": [],
      "source": [
        "#Before making a prediction we first have to normalize our data\n",
        "X_train_normalized,mean,std_dev = z_score_normalization(X_train_l)\n",
        "X_test_normalized = (X_test -mean)/std_dev\n",
        "nn_l.X = X_test_normalized\n",
        "Y_test_prediction = nn_l.predict()\n",
        "#We also append the id's of data with our prediction\n",
        "prediction = np.append(test_id,Y_test_prediction,axis = 1)\n",
        "print(prediction[:5])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v9LVyqeRr1L8"
      },
      "source": [
        "Now finally we export our prediction back into a csv file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mZyxASXwr1L8"
      },
      "outputs": [],
      "source": [
        "dfp=pd.DataFrame(prediction) #Convert matrix to dataframe\n",
        "dfp.columns = [\"ids\",\"prediction\"] #Set labels to our dataframe\n",
        "dfp.to_csv(\"neural_network_linear_test_prediction.csv\",index=False)#Convert dataframe to csv file"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_0_RMVkAr1L8"
      },
      "source": [
        "Also for ease of acess we store our trained w and b in npy files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jDEjXRa4r1L8"
      },
      "outputs": [],
      "source": [
        "#These files can later be loaded using np.load()\n",
        "np.savez(\"neural_network_linear_w\",*nn_l.w)\n",
        "np.savez(\"neural_network_linear_b\",*nn_l.b)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xvEOQcB5r1L8"
      },
      "outputs": [],
      "source": [
        "w_ans = np.load(\"neural_network_linear_w.npz\")\n",
        "b_ans = np.load(\"neural_network_linear_b.npz\")\n",
        "for i in range(len(w_ans)):\n",
        "  print(f\"Weight {i}:-\\n\",w_ans[f'arr_{i}'])\n",
        "  print(f\"Bias {i}:-\\n\",b_ans[f'arr_{i}'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4WQrp56J8KDB"
      },
      "source": [
        "###Regression neural network for polynomial data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZgjVh_OnIxeA"
      },
      "outputs": [],
      "source": [
        "def gen_w_b(X,units):\n",
        "  W_list = []\n",
        "  b_list = []\n",
        "  m,n = X.shape #m is no.of training examples\n",
        "  np.random.seed(793)\n",
        "  for i in range(len(units)):\n",
        "    if i==0:\n",
        "      W_list.append(1*(np.random.randn(n,units[0])-0.5))\n",
        "    else:\n",
        "      W_list.append(1*(np.random.randn(units[i-1],units[i])-0.5))\n",
        "    b_list.append((1*np.random.randn(1,units[i])-0.5))\n",
        "  return W_list,b_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iiYJUeKBOOfP"
      },
      "outputs": [],
      "source": [
        "print(X_train_pmn.shape)\n",
        "m,n = X_train_pmn.shape\n",
        "print(0.8*m)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YotR09o-6UDB"
      },
      "outputs": [],
      "source": [
        "#Defining the network\n",
        "nn_p = NeuralNetwork(X_train_pmn,Y_train_p,[50,20,1],[\"relu\",\"relu\",\"linear\"],\"mean_squared_error\",0.000000001,100,0.8,0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SBVB6J_98kQp"
      },
      "outputs": [],
      "source": [
        "#Prediction\n",
        "print(nn_p.predict())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cSSpTkBG8qPC"
      },
      "outputs": [],
      "source": [
        "#Computing cost\n",
        "print(nn_p.compute_cost())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qrhhhmkM8u3J"
      },
      "outputs": [],
      "source": [
        "#Running gradient descent\n",
        "nn_p.reset_model()\n",
        "nn_p.w,nn_p.b,J_train_history,J_cv_history = nn_p.gradient_descent()\n",
        "plt.figure(figsize=(12,4))\n",
        "plt.plot(J_train_history , label = \"J train\")\n",
        "plt.plot(J_cv_history , label = \"J cv\")\n",
        "plt.title(\"Cost v.s iteration\")\n",
        "plt.ylabel(\"Cost\")\n",
        "plt.xlabel(\"Number of iterations\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "huJBgx5zWBqq"
      },
      "outputs": [],
      "source": [
        "nn_p.w,nn_p.b,J_train_history,J_cv_history = nn_p.gradient_descent()\n",
        "plt.figure(figsize=(12,4))\n",
        "plt.plot(J_train_history , label = \"J train\")\n",
        "plt.plot(J_cv_history , label = \"J cv\")\n",
        "plt.title(\"Cost v.s iteration\")\n",
        "plt.ylabel(\"Cost\")\n",
        "plt.xlabel(\"Number of iterations\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pINiTKT7dG0l"
      },
      "outputs": [],
      "source": [
        "nn_p.iters = 1000\n",
        "nn_p.w,nn_p.b,J_train_history,J_cv_history = nn_p.gradient_descent()\n",
        "plt.figure(figsize=(12,4))\n",
        "plt.plot(J_train_history , label = \"J train\")\n",
        "plt.plot(J_cv_history , label = \"J cv\")\n",
        "plt.title(\"Cost v.s iteration\")\n",
        "plt.ylabel(\"Cost\")\n",
        "plt.xlabel(\"Number of iterations\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QIECDrqsYl3y"
      },
      "outputs": [],
      "source": [
        "nn_p.iters = 1000\n",
        "nn_p.w,nn_p.b,J_train_history,J_cv_history = nn_p.gradient_descent()\n",
        "plt.figure(figsize=(12,4))\n",
        "plt.plot(J_train_history , label = \"J train\")\n",
        "plt.plot(J_cv_history , label = \"J cv\")\n",
        "plt.title(\"Cost v.s iteration\")\n",
        "plt.ylabel(\"Cost\")\n",
        "plt.xlabel(\"Number of iterations\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IomJ_QYEgdpX"
      },
      "outputs": [],
      "source": [
        "nn_p.iters = 1000\n",
        "nn_p.w,nn_p.b,J_train_history,J_cv_history = nn_p.gradient_descent()\n",
        "plt.figure(figsize=(12,4))\n",
        "plt.plot(J_train_history , label = \"J train\")\n",
        "plt.plot(J_cv_history , label = \"J cv\")\n",
        "plt.title(\"Cost v.s iteration\")\n",
        "plt.ylabel(\"Cost\")\n",
        "plt.xlabel(\"Number of iterations\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CLDcbNEapMqH"
      },
      "outputs": [],
      "source": [
        "nn_p.iters = 1000\n",
        "nn_p.w,nn_p.b,J_train_history,J_cv_history = nn_p.gradient_descent()\n",
        "plt.figure(figsize=(12,4))\n",
        "plt.plot(J_train_history , label = \"J train\")\n",
        "plt.plot(J_cv_history , label = \"J cv\")\n",
        "plt.title(\"Cost v.s iteration\")\n",
        "plt.ylabel(\"Cost\")\n",
        "plt.xlabel(\"Number of iterations\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fZZABSLOjsDK"
      },
      "outputs": [],
      "source": [
        "nn_p.alpha = 0.0000000001\n",
        "nn_p.iters = 1000\n",
        "nn_p.w,nn_p.b,J_train_history,J_cv_history = nn_p.gradient_descent()\n",
        "plt.figure(figsize=(12,4))\n",
        "plt.plot(J_train_history , label = \"J train\")\n",
        "plt.plot(J_cv_history , label = \"J cv\")\n",
        "plt.title(\"Cost v.s iteration\")\n",
        "plt.ylabel(\"Cost\")\n",
        "plt.xlabel(\"Number of iterations\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Computing accuracy"
      ],
      "metadata": {
        "id": "Zz-rQ_ERK4pr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Computing accuracy\n",
        "print(\"Accuracy:\",nn_p.accuracy()*100,\"%\")"
      ],
      "metadata": {
        "id": "IRIrPTDGKfZg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6NW3RNw1IT89"
      },
      "source": [
        "###Making a prediction on test dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4H-BHfoqRGyi"
      },
      "outputs": [],
      "source": [
        "#Now we load and print our test data\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/polynomial_test_data.csv\") #Loading data from google drive\n",
        "polynomial_test_data = np.array(df) #Converting loaded data in a matrix\n",
        "test_id_p = polynomial_test_data[:,0] #Getting the Id's of all labels\n",
        "test_id_p = test_id_p.reshape(test_id_p.size,1)\n",
        "X_test_p = polynomial_test_data[:,1:] #Extracting X_test\n",
        "# Printing the first 5 rows of our loaded data\n",
        "print(\"Input:-\\n\",X_test_p[:5])\n",
        "print(\"Shape of test data:-\",X_test_p.shape)\n",
        "print(\"Test Id's:-\\n\",test_id_p[:5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LHdyrgBBRpUM"
      },
      "outputs": [],
      "source": [
        "#Before making a prediction we first have to normalize our data\n",
        "X_train_pm = get_terms(X_train_p,5)\n",
        "X_train_pmn,mean,std_dev = z_score_normalization(X_train_pm)\n",
        "X_test_pm = get_terms(X_test_p,5)\n",
        "X_test_pmn = (X_test_pm -mean)/std_dev\n",
        "nn_p.X = X_test_pmn\n",
        "Y_test_prediction = nn_p.predict()\n",
        "#We also append the id's of data with our prediction\n",
        "prediction = np.append(test_id_p,Y_test_prediction,axis = 1)\n",
        "print(prediction[:5])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o-IN68tBRpUM"
      },
      "source": [
        "Now finally we export our prediction back into a csv file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zsvc0UJ2RpUM"
      },
      "outputs": [],
      "source": [
        "dfp=pd.DataFrame(prediction) #Convert matrix to dataframe\n",
        "dfp.columns = [\"ids\",\"prediction\"] #Set labels to our dataframe\n",
        "dfp.to_csv(\"neural_network_polynomial_test_prediction.csv\",index=False)#Convert dataframe to csv file"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z_sJbhguRpUM"
      },
      "source": [
        "Also for ease of acess we store our trained parameters w and b in npy files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mIyn2XzgRpUM"
      },
      "outputs": [],
      "source": [
        "#These files can later be loaded using np.load()\n",
        "np.savez(\"neural_network_polynomial_w\",*nn_p.w)\n",
        "np.savez(\"neural_network_polynomial_b\",*nn_p.b)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uwJ65VkWRpUM"
      },
      "outputs": [],
      "source": [
        "w_ans = np.load(\"neural_network_polynomial_w.npz\")\n",
        "b_ans = np.load(\"neural_network_polynomial_b.npz\")\n",
        "for i in range(len(w_ans)):\n",
        "  print(f\"Weight {i}:-\\n\",w_ans[f'arr_{i}'])\n",
        "  print(f\"Bias {i}:-\\n\",b_ans[f'arr_{i}'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jg8m9Pe3IaqY"
      },
      "source": [
        "###Classification neural network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PfgC1Hr2Ifal"
      },
      "outputs": [],
      "source": [
        "print(X_train_cn.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8tmEvmHyAoJ7"
      },
      "outputs": [],
      "source": [
        "def gen_w_b(X,units):\n",
        "  W_list = []\n",
        "  b_list = []\n",
        "  m,n = X.shape #m is no.of training examples\n",
        "  np.random.seed(793)\n",
        "  for i in range(len(units)):\n",
        "    if i==0:\n",
        "      W_list.append(0.1*(np.random.randn(n,units[0])-0.5))\n",
        "    else:\n",
        "      W_list.append(0.1*(np.random.randn(units[i-1],units[i])-0.5))\n",
        "    b_list.append((1*np.random.randn(1,units[i])-0.5))\n",
        "  return W_list,b_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g9JuWKLJ_qAN"
      },
      "outputs": [],
      "source": [
        "#Defining the network\n",
        "nn_c = NeuralNetwork(X_train_cn,Y_train_c,[10,10,10],[\"relu\",\"relu\",\"softmax\"],\"cross_entropy\",0.1,1000,0.8,0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MzPIq5IXAQY8"
      },
      "outputs": [],
      "source": [
        "#Prediction\n",
        "print(nn_c.predict())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6-boZEZ4AfjV"
      },
      "outputs": [],
      "source": [
        "#Computing cost\n",
        "print(nn_c.compute_cost())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eEZ7zfeqBefA"
      },
      "outputs": [],
      "source": [
        "#Running Gradient descent\n",
        "nn_c.reset_model()\n",
        "nn_c.w,nn_c.b,J_train_history,J_cv_history = nn_c.gradient_descent()\n",
        "plt.figure(figsize=(12,4))\n",
        "plt.plot(J_train_history , label = \"J train\")\n",
        "plt.plot(J_cv_history , label = \"J cv\")\n",
        "plt.title(\"Cost v.s iteration\")\n",
        "plt.ylabel(\"Cost\")\n",
        "plt.xlabel(\"Number of iterations\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cV1BMdtxl-U-"
      },
      "outputs": [],
      "source": [
        "nn_c.alpha=0.1\n",
        "nn_c.w,nn_c.b,J_train_history,J_cv_history = nn_c.gradient_descent()\n",
        "plt.figure(figsize=(12,4))\n",
        "plt.plot(J_train_history , label = \"J train\")\n",
        "plt.plot(J_cv_history , label = \"J cv\")\n",
        "plt.title(\"Cost v.s iteration\")\n",
        "plt.ylabel(\"Cost\")\n",
        "plt.xlabel(\"Number of iterations\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TUTa96hfDNC3"
      },
      "outputs": [],
      "source": [
        "nn_c.alpha=0.01\n",
        "nn_c.w,nn_c.b,J_train_history,J_cv_history = nn_c.gradient_descent()\n",
        "plt.figure(figsize=(12,4))\n",
        "plt.plot(J_train_history , label = \"J train\")\n",
        "plt.plot(J_cv_history , label = \"J cv\")\n",
        "plt.title(\"Cost v.s iteration\")\n",
        "plt.ylabel(\"Cost\")\n",
        "plt.xlabel(\"Number of iterations\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OyeAhVrXEzEO"
      },
      "outputs": [],
      "source": [
        "nn_c.alpha=0.01\n",
        "nn_c.iters=1000\n",
        "nn_c.w,nn_c.b,J_train_history,J_cv_history = nn_c.gradient_descent()\n",
        "plt.figure(figsize=(12,4))\n",
        "plt.plot(J_train_history , label = \"J train\")\n",
        "plt.plot(J_cv_history , label = \"J cv\")\n",
        "plt.title(\"Cost v.s iteration\")\n",
        "plt.ylabel(\"Cost\")\n",
        "plt.xlabel(\"Number of iterations\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QwfYreEUGpxs"
      },
      "outputs": [],
      "source": [
        "nn_c.alpha=0.01\n",
        "nn_c.iters = 1000\n",
        "nn_c.w,nn_c.b,J_train_history,J_cv_history = nn_c.gradient_descent()\n",
        "plt.figure(figsize=(12,4))\n",
        "plt.plot(J_train_history , label = \"J train\")\n",
        "plt.plot(J_cv_history , label = \"J cv\")\n",
        "plt.title(\"Cost v.s iteration\")\n",
        "plt.ylabel(\"Cost\")\n",
        "plt.xlabel(\"Number of iterations\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UCF9zxBEOFDG"
      },
      "source": [
        "###Visualizing the performance of the classification neural network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qB8HVChYOfM8"
      },
      "outputs": [],
      "source": [
        "for i in range(9):\n",
        "  data = X_train_c[i].reshape(28,28)\n",
        "  plt.imshow(data,cmap=\"Greys_r\")\n",
        "  Y_nn = nn_c.predict()\n",
        "  plt.title(f\"label:{Y_train_c[i]} Neural Network:{Y_nn[i]}\")\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Computing accuracy"
      ],
      "metadata": {
        "id": "35iYRJOHKq8_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Computing accuracy\n",
        "print(\"Accuracy:\",nn_c.accuracy()*100,\"%\")"
      ],
      "metadata": {
        "id": "5aG7oXIVKoYJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JlnzoxqI99g5"
      },
      "source": [
        "###Making a prediction on test dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NLSmmWVB-Cp6"
      },
      "outputs": [],
      "source": [
        "#Now we load and print our test data\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/classification_test.csv\") #Loading data from google drive\n",
        "classification_test_data = np.array(df) #Converting loaded data in a matrix\n",
        "test_id_c = classification_test_data[:,0] #Getting the Id's of all labels\n",
        "test_id_c = test_id_c.reshape(test_id_c.size,1)\n",
        "X_test_c = classification_test_data[:,1:] #Extracting X_test\n",
        "# Printing the first 5 rows of our loaded data\n",
        "print(\"Input:-\\n\",X_test_c[:5])\n",
        "print(\"Shape of test data:-\",X_test_c.shape)\n",
        "print(\"Test Id's:-\\n\",test_id_c[:5])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Before making a prediction we first have to normalize our data\n",
        "X_train_cn,mean,std_dev = z_score_normalization(X_train_c)\n",
        "X_test_normalized = (X_test_c - mean)/std_dev\n",
        "nn_c.X = X_test_normalized\n",
        "Y_test_prediction = nn_c.predict()\n",
        "#We also append the id's of data with our prediction\n",
        "prediction = np.append(test_id_c,Y_test_prediction,axis = 1)\n",
        "print(prediction[:5])"
      ],
      "metadata": {
        "id": "VhVYXYmXi9j6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dfp=pd.DataFrame(prediction) #Convert matrix to dataframe\n",
        "dfp.columns = [\"ids\",\"prediction\"] #Set labels to our dataframe\n",
        "dfp.to_csv(\"neural_network_classification_test_prediction.csv\",index=False)#Convert dataframe to csv file"
      ],
      "metadata": {
        "id": "12synRPfk6o5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yL4MmIK6lk-m"
      },
      "outputs": [],
      "source": [
        "#These files can later be loaded using np.load()\n",
        "np.savez(\"neural_network_classification_w\",*nn_c.w)\n",
        "np.savez(\"neural_network_classification_b\",*nn_c.b)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MBaAxdwxlk-m"
      },
      "outputs": [],
      "source": [
        "w_ans = np.load(\"neural_network_classification_w.npz\")\n",
        "b_ans = np.load(\"neural_network_classification_b.npz\")\n",
        "for i in range(len(w_ans)):\n",
        "  print(f\"Weight {i}:-\\n\",w_ans[f'arr_{i}'])\n",
        "  print(f\"Bias {i}:-\\n\",b_ans[f'arr_{i}'])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}